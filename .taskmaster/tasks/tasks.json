{
  "master": {
    "tasks": [
      {
        "id": 49,
        "title": "Project Initialization and Environment Setup",
        "description": "Set up the project repository, initialize Python environment, and configure Botasaurus for anti-detection scraping.",
        "details": "Create a new Python project with a virtual environment. Install Botasaurus (latest version via pip), and set up project structure with directories for core modules (search, anti-detection, extraction, data, config). Add Dockerfile for containerization. Ensure compatibility with Windows, Mac, and Linux. Prepare .env for secure config and secrets. Use Python 3.10+ for best library support. Add pre-commit hooks for linting and formatting.",
        "testStrategy": "Verify environment setup by running a sample Botasaurus script that opens a browser and navigates to a test page. Confirm Docker container builds and runs locally.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 50,
        "title": "Implement Anti-Detection Engine",
        "description": "Develop the AntiDetectionManager to manage user agent rotation, browser fingerprinting, proxy rotation, and resource blocking using Botasaurus.",
        "status": "in-progress",
        "dependencies": [
          49
        ],
        "priority": "high",
        "details": "Leverage Botasaurus's AntiDetectDriver with @browser(user_agent_rotation=True) for user agent rotation. For consistent fingerprinting, use user_agent=UserAgent.HASHED. Integrate proxy rotation with built-in proxy support (proxy='http://user:pass@host:port') for residential proxies, configurable via YAML/JSON with environment variable templating. Block unnecessary resources using Botasaurus parameters (block_images=True, block_css=True, etc.). Implement session profile management for isolation with profile='profile_name'. Add human-like jittered delays (3-6 seconds random) and implement exponential backoff on 429/403 errors (5s, 10s, 20s, up to 2 minutes). Always respect Retry-After headers and mark proxies unhealthy on repeated blocks. Store proxy credentials securely in .env files (PROXY_USER, PROXY_PASS, etc.).",
        "testStrategy": "Unit test anti-detection features by scraping a honeypot site and verifying no detection. Log user agent and IP for each session. Confirm resource blocking via network logs. Test rate limiting with Bing (15-20 requests/minute) to ensure staying under detection thresholds.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement user agent rotation with Botasaurus",
            "description": "Set up @browser(user_agent_rotation=True) and UserAgent.HASHED for consistent fingerprinting",
            "status": "in-progress",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate proxy rotation system",
            "description": "Implement proxy rotation using Botasaurus's built-in proxy support with proxy='http://user:pass@host:port' format",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Set up resource blocking",
            "description": "Configure Botasaurus resource blocking with block_images=True, block_css=True parameters",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement session profile management",
            "description": "Use profile='campaign_name' for session isolation and consistent fingerprinting across sessions",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add human-like delays and backoff strategy",
            "description": "Implement jittered delays (3-6 seconds) and exponential backoff on 429/403 errors (5s, 10s, 20s, up to 2 minutes)",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create proxy health monitoring",
            "description": "Implement system to mark proxies as unhealthy after repeated blocks and respect Retry-After headers",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Set up secure credential storage",
            "description": "Store proxy credentials in .env files and implement environment variable templating for config files",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Create YAML/JSON configuration system",
            "description": "Develop configuration system with environment variable templating for anti-detection parameters",
            "status": "todo",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 51,
        "title": "Develop Bing Search Interface",
        "description": "Create BingSearchScraper to perform Bing queries, handle pagination, and extract business website URLs using stable CSS selectors.",
        "details": "Use Botasaurus browser automation to navigate Bing search results. Parse results with CSS selectors (e.g., 'li.b_algo h2 a' for URLs). Handle Bing's redirect patterns (/url?q=). Support pagination (next page navigation, up to configurable max pages). Filter out non-business results using a blacklist (Wikipedia, social, etc.). Accept search queries from config files. Implement concurrency limits (max 1-2 sessions for Bing).",
        "testStrategy": "Unit test extraction logic with static Bing HTML. Integration test with live Bing queries, verifying correct URLs and pagination. Validate filtering of non-business results.",
        "priority": "high",
        "dependencies": [
          50
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 52,
        "title": "Implement Rate Limiting and Block Handling",
        "description": "Develop RateLimiter to enforce Bing's request limits, handle HTTP 429 errors, and adaptively manage delays and retries.",
        "details": "Monitor request rates (max 15-20/minute). On 429 or block, trigger exponential backoff (e.g., 30s, 60s, 120s). Adjust delays based on response times. Limit concurrent sessions to 1-2. Detect blocks via response content and switch proxies or tactics. Log all rate limiting and block events for analytics.",
        "testStrategy": "Simulate high-frequency requests to trigger rate limiting. Verify backoff and recovery. Unit test adaptive delay logic. Integration test with Bing to confirm low block rates.",
        "priority": "high",
        "dependencies": [
          51
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 53,
        "title": "Develop Website Email Extraction Engine",
        "description": "Build EmailExtractor to visit business websites, bypass Cloudflare, and extract/validate emails using multiple strategies.",
        "details": "Use Botasaurus with anti-detect features to visit each website. Implement Cloudflare bypass (Botasaurus supports this natively). Extract emails via regex, mailto links, and parsing contact sections. Discover and scrape contact pages (look for '/contact', '/about'). Validate emails (regex, domain checks, filter test/fake emails). Score email quality (e.g., presence on contact page, domain match). Support Shopify, WordPress, and custom site structures.",
        "testStrategy": "Unit test extraction on static HTML samples (Shopify, WP, custom). Integration test on live sites. Validate email scoring and filtering accuracy.",
        "priority": "high",
        "dependencies": [
          52
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 54,
        "title": "Implement Data Pipeline and CSV Export",
        "description": "Create DataExporter to process, deduplicate, validate, and export results to CSV and JSON formats with required fields.",
        "details": "Aggregate extracted emails with metadata (email, source_website, search_query, found_date, confidence_score, website_type, business_name, phone, address). Remove duplicates. Clean and validate data. Export to CSV (using pandas or csv module) and JSON (for campaign summary, proxy stats, error logs). Ensure output matches required schema. Support batch and streaming export modes.",
        "testStrategy": "Unit test data cleaning, deduplication, and export functions. Validate CSV/JSON output against schema. Integration test with full pipeline.",
        "priority": "medium",
        "dependencies": [
          53
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 55,
        "title": "Configuration and Campaign Management",
        "description": "Implement configuration system for campaign settings, search queries, proxies, and anti-detection parameters.",
        "details": "Support YAML/JSON config files for campaign setup. Allow template-based query generation (with city, industry, etc.). Enable proxy pool and rotation settings. Expose anti-detection and rate limiting parameters for tuning. Secure sensitive config (proxy creds) via .env and encrypted storage. Provide CLI for campaign selection and config validation.",
        "testStrategy": "Unit test config parsing and validation. Integration test campaign switching and parameter overrides. Fuzz test with malformed configs.",
        "priority": "medium",
        "dependencies": [
          54
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 56,
        "title": "Monitoring, Logging, and Analytics",
        "description": "Add comprehensive logging, error tracking, and real-time analytics dashboard for campaign performance and proxy health.",
        "details": "Implement structured logging (JSON logs, error_log.csv). Track key metrics (queries/hour, emails/hour, block rates, proxy stats). Export campaign_summary.json and proxy_performance.json. Integrate with monitoring tools (Prometheus, Grafana, or lightweight dashboard using Dash/Streamlit). Alert on high block rates or proxy failures.",
        "testStrategy": "Unit test logging and metric collection. Integration test dashboard with simulated campaigns. Validate alert triggers.",
        "priority": "medium",
        "dependencies": [
          55
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 57,
        "title": "Testing Suite: Unit, Integration, and Performance Tests",
        "description": "Develop comprehensive test suite covering all modules, including anti-detection, extraction, rate limiting, and full pipeline.",
        "details": "Use pytest for unit and integration tests. Mock Bing and website responses for deterministic tests. Add performance tests (load 100+ queries, memory profiling, long-running stability). Test proxy rotation and failover. Validate multi-vertical campaign support. Ensure >90% code coverage.",
        "testStrategy": "Run automated tests on CI. Review coverage reports. Manually verify edge cases and error recovery.",
        "priority": "medium",
        "dependencies": [
          56
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 58,
        "title": "Production Deployment and Cloud Scaling",
        "description": "Containerize application with Docker, enable cloud deployment, and support distributed scraping across multiple machines.",
        "details": "Write Dockerfile for reproducible builds. Support environment-based config (dev, prod). Enable deployment to AWS/GCP/Azure (ECS, GKE, etc.). Implement distributed scraping via message queue (e.g., Redis, RabbitMQ) or orchestrator. Document deployment steps. Integrate monitoring and alerting in production.",
        "testStrategy": "Deploy to cloud test environment. Run distributed scraping campaign. Monitor resource usage, scaling, and recovery from failures.",
        "priority": "medium",
        "dependencies": [
          57
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-19T15:35:53.275Z",
      "updated": "2025-08-21T00:24:18.691Z",
      "description": "Tasks for master context"
    }
  }
}