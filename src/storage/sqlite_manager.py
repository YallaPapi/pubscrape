"""
SQLite Manager for Persistent Storage

Handles SQLite database operations with schema management, 
indexing, and optimized queries for scraped data.
"""

import sqlite3
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
import threading


@dataclass
class StorageResult:
    """Result of storage operation"""
    success: bool
    records_stored: int
    database_path: str
    error_message: Optional[str] = None
    table_name: Optional[str] = None


@dataclass
class QueryResult:
    """Result of database query"""
    success: bool
    data: List[Dict[str, Any]]
    total_count: int
    query_time: float
    error_message: Optional[str] = None


class SQLiteManager:
    """SQLite database manager for lead storage"""
    
    # Standard schema for leads table
    LEADS_SCHEMA = '''
        CREATE TABLE IF NOT EXISTS leads (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            lead_id TEXT NOT NULL UNIQUE,
            business_name TEXT NOT NULL,
            primary_email TEXT,
            primary_phone TEXT,
            contact_name TEXT,
            website TEXT,
            secondary_email TEXT,
            secondary_phone TEXT,
            address TEXT,
            city TEXT,
            state TEXT,
            zip_code TEXT,
            industry TEXT,
            company_size TEXT,
            description TEXT,
            lead_score REAL DEFAULT 0.0,
            is_actionable BOOLEAN DEFAULT 0,
            extraction_date TIMESTAMP,
            source_query TEXT,
            processing_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            processor_session TEXT,
            validation_score REAL,
            dedupe_status TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    '''\n    \n    # Indices for performance\n    INDICES = [\n        'CREATE INDEX IF NOT EXISTS idx_lead_id ON leads(lead_id)',\n        'CREATE INDEX IF NOT EXISTS idx_business_name ON leads(business_name)',\n        'CREATE INDEX IF NOT EXISTS idx_primary_email ON leads(primary_email)',\n        'CREATE INDEX IF NOT EXISTS idx_website ON leads(website)',\n        'CREATE INDEX IF NOT EXISTS idx_lead_score ON leads(lead_score)',\n        'CREATE INDEX IF NOT EXISTS idx_is_actionable ON leads(is_actionable)',\n        'CREATE INDEX IF NOT EXISTS idx_extraction_date ON leads(extraction_date)',\n        'CREATE INDEX IF NOT EXISTS idx_created_at ON leads(created_at)',\n        'CREATE INDEX IF NOT EXISTS idx_processor_session ON leads(processor_session)'\n    ]\n    \n    def __init__(self, db_path: str = None):\n        self.db_path = db_path or \"data/leads.db\"\n        self.logger = logging.getLogger(__name__)\n        self._lock = threading.Lock()\n        \n        # Ensure database directory exists\n        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)\n        \n        # Initialize database\n        self._init_database()\n    \n    def _init_database(self):\n        \"\"\"Initialize database with schema and indices\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute('PRAGMA foreign_keys = ON')\n                conn.execute('PRAGMA journal_mode = WAL')\n                conn.execute('PRAGMA synchronous = NORMAL')\n                conn.execute('PRAGMA cache_size = -64000')  # 64MB cache\n                conn.execute('PRAGMA temp_store = MEMORY')\n                \n                # Create main table\n                conn.execute(self.LEADS_SCHEMA)\n                \n                # Create indices\n                for index_sql in self.INDICES:\n                    conn.execute(index_sql)\n                \n                # Create sessions tracking table\n                conn.execute('''\n                    CREATE TABLE IF NOT EXISTS processing_sessions (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        session_id TEXT NOT NULL UNIQUE,\n                        session_name TEXT,\n                        start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        end_time TIMESTAMP,\n                        total_records INTEGER DEFAULT 0,\n                        valid_records INTEGER DEFAULT 0,\n                        duplicates_removed INTEGER DEFAULT 0,\n                        processing_config TEXT,\n                        status TEXT DEFAULT 'active'\n                    )\n                ''')\n                \n                conn.commit()\n                \n            self.logger.info(f\"SQLite database initialized at {self.db_path}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize database: {str(e)}\")\n            raise\n    \n    async def store_data(self, data: List[Dict[str, Any]], \n                        db_path: str = None,\n                        session_name: str = None,\n                        batch_size: int = 1000) -> bool:\n        \"\"\"Store data in SQLite database\"\"\"\n        \n        target_db = db_path or self.db_path\n        \n        if not data:\n            self.logger.warning(\"No data to store\")\n            return False\n        \n        try:\n            with self._lock:\n                # Use separate connection for this operation if different path\n                if target_db != self.db_path:\n                    # Initialize target database if different\n                    self._init_target_database(target_db)\n                    conn = sqlite3.connect(target_db)\n                else:\n                    conn = sqlite3.connect(self.db_path)\n                \n                try:\n                    conn.execute('BEGIN TRANSACTION')\n                    \n                    # Store in batches for better performance\n                    stored_count = 0\n                    \n                    for i in range(0, len(data), batch_size):\n                        batch = data[i:i + batch_size]\n                        batch_stored = self._store_batch(conn, batch)\n                        stored_count += batch_stored\n                        \n                        self.logger.debug(f\"Stored batch {i//batch_size + 1}: {batch_stored} records\")\n                    \n                    # Update session info\n                    if session_name:\n                        self._update_session_info(conn, session_name, len(data), stored_count)\n                    \n                    conn.execute('COMMIT')\n                    \n                    self.logger.info(f\"Stored {stored_count}/{len(data)} records successfully\")\n                    return True\n                    \n                except Exception as e:\n                    conn.execute('ROLLBACK')\n                    self.logger.error(f\"Transaction rolled back: {str(e)}\")\n                    return False\n                    \n                finally:\n                    conn.close()\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to store data: {str(e)}\")\n            return False\n    \n    def _init_target_database(self, db_path: str):\n        \"\"\"Initialize target database with schema\"\"\"\n        Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n        \n        with sqlite3.connect(db_path) as conn:\n            conn.execute(self.LEADS_SCHEMA)\n            for index_sql in self.INDICES:\n                conn.execute(index_sql)\n            conn.commit()\n    \n    def _store_batch(self, conn: sqlite3.Connection, batch: List[Dict]) -> int:\n        \"\"\"Store a batch of records\"\"\"\n        stored_count = 0\n        \n        for record in batch:\n            try:\n                # Prepare record for storage\n                prepared_record = self._prepare_record(record)\n                \n                # Insert or update record\n                cursor = conn.execute('''\n                    INSERT OR REPLACE INTO leads (\n                        lead_id, business_name, primary_email, primary_phone, \n                        contact_name, website, secondary_email, secondary_phone,\n                        address, city, state, zip_code, industry, company_size,\n                        description, lead_score, is_actionable, extraction_date,\n                        source_query, processing_timestamp, processor_session,\n                        validation_score, dedupe_status, updated_at\n                    ) VALUES (\n                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?\n                    )\n                ''', (\n                    prepared_record['lead_id'],\n                    prepared_record['business_name'],\n                    prepared_record.get('primary_email'),\n                    prepared_record.get('primary_phone'),\n                    prepared_record.get('contact_name'),\n                    prepared_record.get('website'),\n                    prepared_record.get('secondary_email'),\n                    prepared_record.get('secondary_phone'),\n                    prepared_record.get('address'),\n                    prepared_record.get('city'),\n                    prepared_record.get('state'),\n                    prepared_record.get('zip_code'),\n                    prepared_record.get('industry'),\n                    prepared_record.get('company_size'),\n                    prepared_record.get('description'),\n                    prepared_record.get('lead_score', 0.0),\n                    1 if prepared_record.get('is_actionable') else 0,\n                    prepared_record.get('extraction_date'),\n                    prepared_record.get('source_query'),\n                    prepared_record.get('processing_timestamp'),\n                    prepared_record.get('processor_session'),\n                    prepared_record.get('validation_score'),\n                    prepared_record.get('dedupe_status'),\n                    datetime.now().isoformat()\n                ))\n                \n                stored_count += 1\n                \n            except sqlite3.Error as e:\n                self.logger.error(f\"Failed to store record {record.get('lead_id', 'unknown')}: {str(e)}\")\n                continue\n        \n        return stored_count\n    \n    def _prepare_record(self, record: Dict) -> Dict:\n        \"\"\"Prepare record for database storage\"\"\"\n        prepared = record.copy()\n        \n        # Ensure lead_id exists\n        if 'lead_id' not in prepared or not prepared['lead_id']:\n            import hashlib\n            content = json.dumps(record, sort_keys=True, default=str)\n            prepared['lead_id'] = hashlib.md5(content.encode()).hexdigest()[:12]\n        \n        # Ensure business_name exists\n        if 'business_name' not in prepared or not prepared['business_name']:\n            prepared['business_name'] = 'Unknown Business'\n        \n        # Convert boolean fields\n        if 'is_actionable' in prepared:\n            prepared['is_actionable'] = bool(prepared['is_actionable'])\n        \n        # Convert numeric fields\n        for field in ['lead_score', 'validation_score']:\n            if field in prepared and prepared[field] is not None:\n                try:\n                    prepared[field] = float(prepared[field])\n                except (ValueError, TypeError):\n                    prepared[field] = 0.0\n        \n        # Ensure timestamp fields are properly formatted\n        timestamp_fields = ['extraction_date', 'processing_timestamp']\n        for field in timestamp_fields:\n            if field in prepared and prepared[field]:\n                try:\n                    # Try to parse and reformat\n                    if isinstance(prepared[field], str):\n                        dt = datetime.fromisoformat(prepared[field].replace('Z', '+00:00'))\n                        prepared[field] = dt.isoformat()\n                except (ValueError, TypeError):\n                    # Keep original value if parsing fails\n                    pass\n        \n        return prepared\n    \n    def _update_session_info(self, conn: sqlite3.Connection, session_name: str, \n                           total_records: int, stored_records: int):\n        \"\"\"Update processing session information\"\"\"\n        try:\n            conn.execute('''\n                INSERT OR REPLACE INTO processing_sessions \n                (session_id, session_name, total_records, valid_records, status)\n                VALUES (?, ?, ?, ?, 'completed')\n            ''', (session_name, session_name, total_records, stored_records))\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to update session info: {str(e)}\")\n    \n    def query_data(self, filters: Dict[str, Any] = None, \n                  limit: int = None, offset: int = 0,\n                  order_by: str = 'created_at DESC') -> QueryResult:\n        \"\"\"Query data from database with filters\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.row_factory = sqlite3.Row  # Enable column access by name\n                \n                # Build query\n                where_clause, params = self._build_where_clause(filters or {})\n                \n                # Count total records\n                count_query = f\"SELECT COUNT(*) FROM leads{where_clause}\"\n                total_count = conn.execute(count_query, params).fetchone()[0]\n                \n                # Build main query\n                query = f\"SELECT * FROM leads{where_clause} ORDER BY {order_by}\"\n                if limit:\n                    query += f\" LIMIT {limit}\"\n                if offset:\n                    query += f\" OFFSET {offset}\"\n                \n                # Execute query\n                cursor = conn.execute(query, params)\n                rows = cursor.fetchall()\n                \n                # Convert to list of dicts\n                data = [dict(row) for row in rows]\n                \n                query_time = (datetime.now() - start_time).total_seconds()\n                \n                return QueryResult(\n                    success=True,\n                    data=data,\n                    total_count=total_count,\n                    query_time=query_time\n                )\n                \n        except Exception as e:\n            self.logger.error(f\"Query failed: {str(e)}\")\n            return QueryResult(\n                success=False,\n                data=[],\n                total_count=0,\n                query_time=0.0,\n                error_message=str(e)\n            )\n    \n    def _build_where_clause(self, filters: Dict[str, Any]) -> Tuple[str, List]:\n        \"\"\"Build WHERE clause from filters\"\"\"\n        conditions = []\n        params = []\n        \n        for field, value in filters.items():\n            if value is None:\n                continue\n                \n            if field == 'search':\n                # Full-text search across multiple fields\n                search_condition = '''(\n                    business_name LIKE ? OR \n                    primary_email LIKE ? OR \n                    contact_name LIKE ? OR\n                    website LIKE ?\n                )'''\n                search_term = f\"%{value}%\"\n                conditions.append(search_condition)\n                params.extend([search_term, search_term, search_term, search_term])\n                \n            elif field == 'min_score':\n                conditions.append('lead_score >= ?')\n                params.append(float(value))\n                \n            elif field == 'max_score':\n                conditions.append('lead_score <= ?')\n                params.append(float(value))\n                \n            elif field == 'is_actionable':\n                conditions.append('is_actionable = ?')\n                params.append(1 if value else 0)\n                \n            elif field == 'has_email':\n                conditions.append('primary_email IS NOT NULL AND primary_email != \"\"')\n                \n            elif field == 'has_phone':\n                conditions.append('primary_phone IS NOT NULL AND primary_phone != \"\"')\n                \n            elif field == 'date_from':\n                conditions.append('extraction_date >= ?')\n                params.append(value)\n                \n            elif field == 'date_to':\n                conditions.append('extraction_date <= ?')\n                params.append(value)\n                \n            elif field == 'processor_session':\n                conditions.append('processor_session = ?')\n                params.append(value)\n                \n            else:\n                # Direct field match\n                conditions.append(f'{field} = ?')\n                params.append(value)\n        \n        where_clause = ''\n        if conditions:\n            where_clause = ' WHERE ' + ' AND '.join(conditions)\n        \n        return where_clause, params\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get database statistics\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                stats = {}\n                \n                # Total records\n                stats['total_records'] = conn.execute('SELECT COUNT(*) FROM leads').fetchone()[0]\n                \n                # Actionable records\n                stats['actionable_records'] = conn.execute(\n                    'SELECT COUNT(*) FROM leads WHERE is_actionable = 1'\n                ).fetchone()[0]\n                \n                # Records with email\n                stats['records_with_email'] = conn.execute(\n                    'SELECT COUNT(*) FROM leads WHERE primary_email IS NOT NULL AND primary_email != \"\"'\n                ).fetchone()[0]\n                \n                # Records with phone\n                stats['records_with_phone'] = conn.execute(\n                    'SELECT COUNT(*) FROM leads WHERE primary_phone IS NOT NULL AND primary_phone != \"\"'\n                ).fetchone()[0]\n                \n                # Average lead score\n                avg_score = conn.execute('SELECT AVG(lead_score) FROM leads').fetchone()[0]\n                stats['average_lead_score'] = round(avg_score or 0, 3)\n                \n                # Date range\n                date_range = conn.execute(\n                    'SELECT MIN(extraction_date), MAX(extraction_date) FROM leads'\n                ).fetchone()\n                stats['date_range'] = {\n                    'earliest': date_range[0],\n                    'latest': date_range[1]\n                }\n                \n                # Top industries\n                industries = conn.execute('''\n                    SELECT industry, COUNT(*) as count \n                    FROM leads \n                    WHERE industry IS NOT NULL AND industry != \"\"\n                    GROUP BY industry \n                    ORDER BY count DESC \n                    LIMIT 10\n                ''').fetchall()\n                stats['top_industries'] = [{'industry': row[0], 'count': row[1]} for row in industries]\n                \n                # Processing sessions\n                sessions = conn.execute('SELECT COUNT(*) FROM processing_sessions').fetchone()[0]\n                stats['processing_sessions'] = sessions\n                \n                return stats\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to get statistics: {str(e)}\")\n            return {}\n    \n    def export_to_csv(self, output_path: str, filters: Dict[str, Any] = None) -> bool:\n        \"\"\"Export data to CSV file\"\"\"\n        try:\n            result = self.query_data(filters)\n            \n            if not result.success or not result.data:\n                self.logger.warning(\"No data to export\")\n                return False\n            \n            import csv\n            \n            output_file = Path(output_path)\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n                if result.data:\n                    fieldnames = result.data[0].keys()\n                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                    writer.writeheader()\n                    writer.writerows(result.data)\n            \n            self.logger.info(f\"Exported {len(result.data)} records to {output_path}\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Export to CSV failed: {str(e)}\")\n            return False\n    \n    def backup_database(self, backup_path: str) -> bool:\n        \"\"\"Create database backup\"\"\"\n        try:\n            import shutil\n            \n            backup_file = Path(backup_path)\n            backup_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            # Copy database file\n            shutil.copy2(self.db_path, backup_path)\n            \n            self.logger.info(f\"Database backed up to {backup_path}\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Database backup failed: {str(e)}\")\n            return False\n    \n    def optimize_database(self) -> bool:\n        \"\"\"Optimize database performance\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                # Analyze tables for query optimization\n                conn.execute('ANALYZE')\n                \n                # Vacuum to reclaim space\n                conn.execute('VACUUM')\n                \n                # Update statistics\n                conn.execute('REINDEX')\n                \n            self.logger.info(\"Database optimization completed\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Database optimization failed: {str(e)}\")\n            return False\n    \n    def close(self):\n        \"\"\"Close database connections and cleanup\"\"\"\n        # SQLite connections are automatically closed when using context managers\n        self.logger.info(\"SQLite manager closed\")"