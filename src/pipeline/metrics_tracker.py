"""
Metrics Tracker for Pipeline Performance

Tracks success rates, errors, processing times, and data quality
metrics across pipeline operations with detailed reporting.
"""

import json
import time
import logging
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import threading


@dataclass
class ProcessingMetrics:
    """Container for processing metrics"""
    session_id: str
    session_name: str
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Record counts
    total_records: int = 0
    processed_records: int = 0
    valid_records: int = 0
    invalid_records: int = 0
    duplicate_records: int = 0
    enriched_records: int = 0
    
    # Performance metrics
    processing_time: float = 0.0
    avg_processing_time_per_record: float = 0.0
    records_per_second: float = 0.0
    
    # Quality metrics
    validation_success_rate: float = 0.0
    duplication_rate: float = 0.0
    enrichment_rate: float = 0.0
    data_completeness_score: float = 0.0
    
    # Error tracking
    total_errors: int = 0
    validation_errors: int = 0
    processing_errors: int = 0
    export_errors: int = 0
    
    # Memory and resource usage
    peak_memory_usage: float = 0.0
    avg_memory_usage: float = 0.0


@dataclass
class ValidationMetric:
    """Individual validation metric"""
    timestamp: datetime
    record_id: str
    is_valid: bool
    error_count: int
    warning_count: int
    validation_score: float
    processing_time: float


@dataclass
class DeduplicationMetric:
    """Individual deduplication metric"""
    timestamp: datetime
    record_id: str
    is_duplicate: bool
    match_type: str
    similarity_score: float
    duplicate_id: Optional[str] = None


class MetricsTracker:
    """Comprehensive metrics tracking system"""
    
    def __init__(self, storage_path: str = None):
        self.storage_path = Path(storage_path or "data/metrics")
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        self._lock = threading.Lock()
        
        # Active session tracking
        self.current_session: Optional[ProcessingMetrics] = None
        
        # Real-time metrics collection
        self.validation_metrics: List[ValidationMetric] = []
        self.deduplication_metrics: List[DeduplicationMetric] = []
        
        # Error tracking
        self.errors: List[Dict[str, Any]] = []
        self.warnings: List[Dict[str, Any]] = []
        
        # Performance tracking
        self.operation_times: Dict[str, List[float]] = defaultdict(list)
        self.memory_snapshots: List[Tuple[datetime, float]] = []
        
        # Resource monitoring
        self._start_resource_monitoring()
    
    def _start_resource_monitoring(self):
        """Start background resource monitoring"""
        import psutil
        import threading
        
        def monitor_resources():
            while hasattr(self, '_monitoring') and self._monitoring:
                try:\n                    # Get current process\n                    process = psutil.Process()\n                    memory_mb = process.memory_info().rss / 1024 / 1024\n                    \n                    # Store memory snapshot\n                    self.memory_snapshots.append((datetime.now(), memory_mb))\n                    \n                    # Keep only last 1000 snapshots\n                    if len(self.memory_snapshots) > 1000:\n                        self.memory_snapshots = self.memory_snapshots[-1000:]\n                    \n                    time.sleep(5)  # Monitor every 5 seconds\n                    \n                except Exception as e:\n                    self.logger.debug(f\"Resource monitoring error: {str(e)}\")\n                    time.sleep(10)\n        \n        self._monitoring = True\n        self._monitor_thread = threading.Thread(target=monitor_resources, daemon=True)\n        self._monitor_thread.start()\n    \n    async def start_session(self, session_name: str) -> str:\n        \"\"\"Start a new metrics tracking session\"\"\"\n        session_id = f\"{session_name}_{int(time.time())}\"\n        \n        with self._lock:\n            self.current_session = ProcessingMetrics(\n                session_id=session_id,\n                session_name=session_name,\n                start_time=datetime.now()\n            )\n            \n            # Clear previous session data\n            self.validation_metrics.clear()\n            self.deduplication_metrics.clear()\n            self.errors.clear()\n            self.warnings.clear()\n            \n            # Reset operation times for new session\n            self.operation_times.clear()\n        \n        self.logger.info(f\"Metrics tracking started for session: {session_id}\")\n        return session_id\n    \n    async def end_session(self, final_counts: Dict[str, Any] = None):\n        \"\"\"End current metrics tracking session\"\"\"\n        if not self.current_session:\n            self.logger.warning(\"No active session to end\")\n            return\n        \n        with self._lock:\n            # Update final metrics\n            self.current_session.end_time = datetime.now()\n            \n            if final_counts:\n                for key, value in final_counts.items():\n                    if hasattr(self.current_session, key):\n                        setattr(self.current_session, key, value)\n            \n            # Calculate derived metrics\n            self._calculate_derived_metrics()\n            \n            # Save session metrics\n            await self._save_session_metrics()\n        \n        self.logger.info(f\"Metrics tracking ended for session: {self.current_session.session_id}\")\n    \n    def _calculate_derived_metrics(self):\n        \"\"\"Calculate derived metrics from collected data\"\"\"\n        if not self.current_session:\n            return\n        \n        session = self.current_session\n        \n        # Processing time\n        if session.end_time and session.start_time:\n            session.processing_time = (session.end_time - session.start_time).total_seconds()\n        \n        # Per-record metrics\n        if session.processed_records > 0:\n            session.avg_processing_time_per_record = session.processing_time / session.processed_records\n            session.records_per_second = session.processed_records / max(session.processing_time, 0.001)\n        \n        # Quality metrics\n        if session.total_records > 0:\n            session.validation_success_rate = session.valid_records / session.total_records\n            session.duplication_rate = session.duplicate_records / session.total_records\n            session.enrichment_rate = session.enriched_records / session.total_records\n        \n        # Data completeness (from validation metrics)\n        if self.validation_metrics:\n            avg_validation_score = sum(m.validation_score for m in self.validation_metrics) / len(self.validation_metrics)\n            session.data_completeness_score = avg_validation_score\n        \n        # Memory usage\n        if self.memory_snapshots:\n            memory_values = [snapshot[1] for snapshot in self.memory_snapshots]\n            session.peak_memory_usage = max(memory_values)\n            session.avg_memory_usage = sum(memory_values) / len(memory_values)\n    \n    async def record_validation(self, validation_result, processing_time: float = 0.0):\n        \"\"\"Record validation metrics\"\"\"\n        metric = ValidationMetric(\n            timestamp=datetime.now(),\n            record_id=validation_result.record_id,\n            is_valid=validation_result.is_valid,\n            error_count=validation_result.error_count,\n            warning_count=validation_result.warning_count,\n            validation_score=validation_result.score,\n            processing_time=processing_time\n        )\n        \n        with self._lock:\n            self.validation_metrics.append(metric)\n        \n        # Record in operation times\n        self.operation_times['validation'].append(processing_time)\n    \n    def record_validation_sync(self, validation_result, processing_time: float = 0.0):\n        \"\"\"Synchronous version of record_validation\"\"\"\n        metric = ValidationMetric(\n            timestamp=datetime.now(),\n            record_id=validation_result.record_id,\n            is_valid=validation_result.is_valid,\n            error_count=validation_result.error_count,\n            warning_count=validation_result.warning_count,\n            validation_score=validation_result.score,\n            processing_time=processing_time\n        )\n        \n        with self._lock:\n            self.validation_metrics.append(metric)\n        \n        self.operation_times['validation'].append(processing_time)\n    \n    async def record_deduplication(self, dedupe_result, processing_time: float = 0.0):\n        \"\"\"Record deduplication metrics\"\"\"\n        metric = DeduplicationMetric(\n            timestamp=datetime.now(),\n            record_id=dedupe_result.original_record_id,\n            is_duplicate=dedupe_result.is_duplicate,\n            match_type=dedupe_result.match_type,\n            similarity_score=dedupe_result.similarity_score,\n            duplicate_id=dedupe_result.duplicate_id\n        )\n        \n        with self._lock:\n            self.deduplication_metrics.append(metric)\n        \n        self.operation_times['deduplication'].append(processing_time)\n    \n    def record_deduplication_sync(self, dedupe_result, processing_time: float = 0.0):\n        \"\"\"Synchronous version of record_deduplication\"\"\"\n        metric = DeduplicationMetric(\n            timestamp=datetime.now(),\n            record_id=dedupe_result.original_record_id,\n            is_duplicate=dedupe_result.is_duplicate,\n            match_type=dedupe_result.match_type,\n            similarity_score=dedupe_result.similarity_score,\n            duplicate_id=dedupe_result.duplicate_id\n        )\n        \n        with self._lock:\n            self.deduplication_metrics.append(metric)\n        \n        self.operation_times['deduplication'].append(processing_time)\n    \n    async def record_error(self, error_message: str, error_type: str = \"general\", \n                          record_id: str = None, context: Dict = None):\n        \"\"\"Record an error with context\"\"\"\n        error_record = {\n            'timestamp': datetime.now().isoformat(),\n            'error_type': error_type,\n            'message': error_message,\n            'record_id': record_id,\n            'context': context or {},\n            'session_id': self.current_session.session_id if self.current_session else None\n        }\n        \n        with self._lock:\n            self.errors.append(error_record)\n            \n            # Update session error counts\n            if self.current_session:\n                self.current_session.total_errors += 1\n                \n                if error_type == 'validation':\n                    self.current_session.validation_errors += 1\n                elif error_type == 'processing':\n                    self.current_session.processing_errors += 1\n                elif error_type == 'export':\n                    self.current_session.export_errors += 1\n        \n        self.logger.error(f\"[{error_type}] {error_message}\")\n    \n    async def record_warning(self, warning_message: str, warning_type: str = \"general\",\n                           record_id: str = None, context: Dict = None):\n        \"\"\"Record a warning with context\"\"\"\n        warning_record = {\n            'timestamp': datetime.now().isoformat(),\n            'warning_type': warning_type,\n            'message': warning_message,\n            'record_id': record_id,\n            'context': context or {},\n            'session_id': self.current_session.session_id if self.current_session else None\n        }\n        \n        with self._lock:\n            self.warnings.append(warning_record)\n        \n        self.logger.warning(f\"[{warning_type}] {warning_message}\")\n    \n    def record_operation_time(self, operation: str, time_seconds: float):\n        \"\"\"Record operation timing\"\"\"\n        self.operation_times[operation].append(time_seconds)\n    \n    async def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive metrics summary\"\"\"\n        if not self.current_session:\n            return {'error': 'No active session'}\n        \n        with self._lock:\n            # Calculate current derived metrics\n            self._calculate_derived_metrics()\n            \n            # Operation statistics\n            operation_stats = {}\n            for op, times in self.operation_times.items():\n                if times:\n                    operation_stats[op] = {\n                        'count': len(times),\n                        'total_time': sum(times),\n                        'avg_time': sum(times) / len(times),\n                        'min_time': min(times),\n                        'max_time': max(times)\n                    }\n            \n            # Validation statistics\n            validation_stats = self._get_validation_stats()\n            \n            # Deduplication statistics\n            deduplication_stats = self._get_deduplication_stats()\n            \n            # Error analysis\n            error_analysis = self._get_error_analysis()\n            \n            summary = {\n                'session': asdict(self.current_session),\n                'operation_statistics': operation_stats,\n                'validation_statistics': validation_stats,\n                'deduplication_statistics': deduplication_stats,\n                'error_analysis': error_analysis,\n                'memory_usage': {\n                    'peak_mb': self.current_session.peak_memory_usage,\n                    'average_mb': self.current_session.avg_memory_usage,\n                    'snapshots_count': len(self.memory_snapshots)\n                },\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            return summary\n    \n    def _get_validation_stats(self) -> Dict[str, Any]:\n        \"\"\"Get validation statistics\"\"\"\n        if not self.validation_metrics:\n            return {}\n        \n        total_validations = len(self.validation_metrics)\n        valid_count = sum(1 for m in self.validation_metrics if m.is_valid)\n        \n        # Score distribution\n        scores = [m.validation_score for m in self.validation_metrics]\n        \n        # Error/warning distribution\n        total_errors = sum(m.error_count for m in self.validation_metrics)\n        total_warnings = sum(m.warning_count for m in self.validation_metrics)\n        \n        return {\n            'total_validations': total_validations,\n            'valid_records': valid_count,\n            'invalid_records': total_validations - valid_count,\n            'success_rate': valid_count / total_validations,\n            'average_score': sum(scores) / len(scores),\n            'min_score': min(scores),\n            'max_score': max(scores),\n            'total_errors': total_errors,\n            'total_warnings': total_warnings,\n            'avg_errors_per_record': total_errors / total_validations,\n            'avg_warnings_per_record': total_warnings / total_validations\n        }\n    \n    def _get_deduplication_stats(self) -> Dict[str, Any]:\n        \"\"\"Get deduplication statistics\"\"\"\n        if not self.deduplication_metrics:\n            return {}\n        \n        total_checks = len(self.deduplication_metrics)\n        duplicate_count = sum(1 for m in self.deduplication_metrics if m.is_duplicate)\n        \n        # Match type distribution\n        match_types = Counter(m.match_type for m in self.deduplication_metrics if m.is_duplicate)\n        \n        # Similarity score statistics\n        similarity_scores = [m.similarity_score for m in self.deduplication_metrics if m.is_duplicate]\n        \n        return {\n            'total_checks': total_checks,\n            'duplicates_found': duplicate_count,\n            'unique_records': total_checks - duplicate_count,\n            'duplication_rate': duplicate_count / total_checks,\n            'match_type_distribution': dict(match_types),\n            'average_similarity_score': sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0,\n            'min_similarity_score': min(similarity_scores) if similarity_scores else 0,\n            'max_similarity_score': max(similarity_scores) if similarity_scores else 0\n        }\n    \n    def _get_error_analysis(self) -> Dict[str, Any]:\n        \"\"\"Analyze errors and warnings\"\"\"\n        # Error type distribution\n        error_types = Counter(error['error_type'] for error in self.errors)\n        warning_types = Counter(warning['warning_type'] for warning in self.warnings)\n        \n        # Recent errors (last 10)\n        recent_errors = sorted(self.errors, key=lambda x: x['timestamp'], reverse=True)[:10]\n        recent_warnings = sorted(self.warnings, key=lambda x: x['timestamp'], reverse=True)[:10]\n        \n        return {\n            'total_errors': len(self.errors),\n            'total_warnings': len(self.warnings),\n            'error_type_distribution': dict(error_types),\n            'warning_type_distribution': dict(warning_types),\n            'recent_errors': recent_errors,\n            'recent_warnings': recent_warnings\n        }\n    \n    async def _save_session_metrics(self):\n        \"\"\"Save session metrics to file\"\"\"\n        if not self.current_session:\n            return\n        \n        try:\n            # Create session-specific metrics file\n            metrics_file = self.storage_path / f\"{self.current_session.session_id}_metrics.json\"\n            \n            summary = await self.get_summary()\n            \n            with open(metrics_file, 'w', encoding='utf-8') as f:\n                json.dump(summary, f, indent=2, default=str)\n            \n            self.logger.info(f\"Session metrics saved to {metrics_file}\")\n            \n            # Also save to general metrics log\n            await self._append_to_metrics_log(summary)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save session metrics: {str(e)}\")\n    \n    async def _append_to_metrics_log(self, summary: Dict):\n        \"\"\"Append summary to general metrics log\"\"\"\n        try:\n            log_file = self.storage_path / \"metrics_log.jsonl\"\n            \n            with open(log_file, 'a', encoding='utf-8') as f:\n                f.write(json.dumps(summary, default=str) + '\\n')\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to append to metrics log: {str(e)}\")\n    \n    def get_historical_metrics(self, days: int = 30) -> List[Dict[str, Any]]:\n        \"\"\"Get historical metrics from log file\"\"\"\n        try:\n            log_file = self.storage_path / \"metrics_log.jsonl\"\n            \n            if not log_file.exists():\n                return []\n            \n            historical = []\n            cutoff_date = datetime.now().timestamp() - (days * 24 * 3600)\n            \n            with open(log_file, 'r', encoding='utf-8') as f:\n                for line in f:\n                    try:\n                        entry = json.loads(line.strip())\n                        \n                        # Check if entry is within date range\n                        session_start = entry.get('session', {}).get('start_time')\n                        if session_start:\n                            entry_time = datetime.fromisoformat(session_start).timestamp()\n                            if entry_time >= cutoff_date:\n                                historical.append(entry)\n                        else:\n                            historical.append(entry)  # Include if no timestamp\n                            \n                    except json.JSONDecodeError:\n                        continue\n            \n            return historical\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load historical metrics: {str(e)}\")\n            return []\n    \n    def generate_performance_report(self, output_path: str = None) -> str:\n        \"\"\"Generate comprehensive performance report\"\"\"\n        try:\n            report_data = {\n                'report_generated': datetime.now().isoformat(),\n                'current_session': asdict(self.current_session) if self.current_session else None,\n                'historical_data': self.get_historical_metrics(7),  # Last 7 days\n                'summary_statistics': self._get_performance_summary()\n            }\n            \n            # Determine output path\n            if not output_path:\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                output_path = str(self.storage_path / f\"performance_report_{timestamp}.json\")\n            \n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(report_data, f, indent=2, default=str)\n            \n            self.logger.info(f\"Performance report generated: {output_path}\")\n            return output_path\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate performance report: {str(e)}\")\n            return \"\"\n    \n    def _get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get performance summary across sessions\"\"\"\n        historical = self.get_historical_metrics(30)\n        \n        if not historical:\n            return {}\n        \n        # Extract key metrics\n        processing_times = []\n        records_per_second = []\n        validation_rates = []\n        duplication_rates = []\n        \n        for session_data in historical:\n            session_info = session_data.get('session', {})\n            \n            if session_info.get('processing_time'):\n                processing_times.append(session_info['processing_time'])\n            \n            if session_info.get('records_per_second'):\n                records_per_second.append(session_info['records_per_second'])\n            \n            if session_info.get('validation_success_rate'):\n                validation_rates.append(session_info['validation_success_rate'])\n            \n            if session_info.get('duplication_rate'):\n                duplication_rates.append(session_info['duplication_rate'])\n        \n        def safe_avg(lst): return sum(lst) / len(lst) if lst else 0\n        def safe_min(lst): return min(lst) if lst else 0\n        def safe_max(lst): return max(lst) if lst else 0\n        \n        return {\n            'sessions_analyzed': len(historical),\n            'avg_processing_time': safe_avg(processing_times),\n            'avg_records_per_second': safe_avg(records_per_second),\n            'avg_validation_rate': safe_avg(validation_rates),\n            'avg_duplication_rate': safe_avg(duplication_rates),\n            'best_performance_rps': safe_max(records_per_second),\n            'worst_performance_rps': safe_min(records_per_second)\n        }\n    \n    def cleanup(self):\n        \"\"\"Cleanup resources\"\"\"\n        self._monitoring = False\n        \n        # Wait for monitor thread to finish\n        if hasattr(self, '_monitor_thread') and self._monitor_thread.is_alive():\n            self._monitor_thread.join(timeout=1)\n        \n        self.logger.info(\"Metrics tracker cleanup completed\")"