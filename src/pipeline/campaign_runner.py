"""
Campaign Runner for VRSEN PubScrape

Orchestrates complete scraping campaigns with intelligent coordination,
error handling, and progress tracking.
"""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import json
import yaml
import logging
from dataclasses import dataclass, field

from ..config.config_manager import config_manager
from ..core.agency_factory import AgencyFactory
from ..utils.error_handler import ErrorHandler
from ..utils.resume_manager import ResumeManager
from ..utils.performance_monitor import PerformanceMonitor


@dataclass
class CampaignResults:
    """Campaign execution results"""
    session_id: str
    campaign_name: str
    start_time: datetime
    end_time: Optional[datetime] = None
    total_leads: int = 0
    successful_queries: int = 0
    failed_queries: int = 0
    errors: List[Dict[str, Any]] = field(default_factory=list)
    warnings: List[Dict[str, Any]] = field(default_factory=list)
    performance_stats: Dict[str, Any] = field(default_factory=dict)
    output_files: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class CampaignRunner:
    """
    Comprehensive campaign runner with coordination and monitoring.
    
    Features:
    - Multi-agent coordination
    - Intelligent query distribution
    - Real-time progress tracking
    - Error handling and recovery
    - Performance optimization
    - Resume capability
    - Results aggregation and reporting
    """
    
    def __init__(self, 
                 config: Any,
                 agency_factory: AgencyFactory,
                 error_handler: ErrorHandler,
                 resume_manager: ResumeManager,
                 logger: logging.Logger):
        
        self.config = config
        self.agency_factory = agency_factory
        self.error_handler = error_handler
        self.resume_manager = resume_manager
        self.logger = logger
        
        # Campaign state
        self.current_session_id: Optional[str] = None
        self.current_results: Optional[CampaignResults] = None
        self.is_running = False
        self.cancellation_requested = False
        
        # Progress tracking
        self.progress_callbacks: List[Callable] = []
        self.total_queries = 0
        self.completed_queries = 0
        
        self.logger.info("Campaign runner initialized")
    
    async def run(self, campaign_config: Dict[str, Any], 
                  session_id: str,
                  max_leads: Optional[int] = None) -> Dict[str, Any]:
        """
        Run a complete scraping campaign.
        
        Args:
            campaign_config: Campaign configuration
            session_id: Unique session identifier
            max_leads: Maximum leads to generate
            
        Returns:
            Campaign results dictionary
        """
        try:\n            self.logger.info(f\"Starting campaign: {campaign_config.get('name', 'Unnamed')}\")\n            \n            # Initialize campaign\n            self.current_session_id = session_id\n            self.is_running = True\n            self.cancellation_requested = False\n            \n            # Create session\n            session = self.resume_manager.create_session(session_id, campaign_config)\n            self.resume_manager.start_session(session_id)\n            \n            # Initialize results\n            self.current_results = CampaignResults(\n                session_id=session_id,\n                campaign_name=campaign_config.get('name', 'Unnamed Campaign'),\n                start_time=datetime.now()\n            )\n            \n            # Validate campaign configuration\n            validation_result = await self._validate_campaign_config(campaign_config)\n            if not validation_result['valid']:\n                raise ValueError(f\"Invalid campaign configuration: {validation_result['errors']}\")\n            \n            # Initialize performance monitoring\n            perf_monitor = PerformanceMonitor()\n            perf_monitor.start()\n            \n            try:\n                # Preprocess campaign\n                processed_config = await self._preprocess_campaign(campaign_config, max_leads)\n                \n                # Generate query plan\n                query_plan = await self._generate_query_plan(processed_config)\n                self.total_queries = len(query_plan['queries'])\n                \n                self.logger.info(f\"Generated query plan with {self.total_queries} queries\")\n                \n                # Save initial checkpoint\n                self.resume_manager.save_checkpoint({\n                    'stage': 'query_planning_complete',\n                    'query_plan': query_plan,\n                    'processed_config': processed_config\n                }, {\n                    'total_progress': 10.0,\n                    'current_step': 'Query planning complete'\n                })\n                \n                # Initialize agency swarm\n                agency = await self._initialize_agency(processed_config)\n                \n                # Execute queries with coordination\n                leads_results = await self._execute_query_plan(\n                    query_plan, agency, processed_config\n                )\n                \n                # Process and validate leads\n                validated_leads = await self._process_leads(leads_results, processed_config)\n                \n                # Export results\n                output_files = await self._export_results(validated_leads, processed_config)\n                \n                # Finalize results\n                self.current_results.end_time = datetime.now()\n                self.current_results.total_leads = len(validated_leads)\n                self.current_results.output_files = output_files\n                self.current_results.performance_stats = perf_monitor.get_metrics_summary()\n                \n                # Complete session\n                results_dict = {\n                    'leads': validated_leads,\n                    'total_leads': len(validated_leads),\n                    'successful_queries': self.current_results.successful_queries,\n                    'failed_queries': self.current_results.failed_queries,\n                    'output_files': output_files,\n                    'performance_stats': self.current_results.performance_stats,\n                    'errors': self.current_results.errors,\n                    'warnings': self.current_results.warnings\n                }\n                \n                self.resume_manager.complete_session(session_id, results_dict)\n                \n                self.logger.info(f\"Campaign completed successfully: {len(validated_leads)} leads generated\")\n                return results_dict\n                \n            finally:\n                perf_monitor.stop()\n                self.is_running = False\n                \n        except Exception as e:\n            self.logger.error(f\"Campaign execution failed: {e}\", exc_info=True)\n            \n            if self.current_results:\n                self.current_results.errors.append({\n                    'timestamp': datetime.now().isoformat(),\n                    'error': str(e),\n                    'stage': 'campaign_execution'\n                })\n            \n            # Fail session\n            if self.current_session_id:\n                self.resume_manager.fail_session(self.current_session_id, {\n                    'error': str(e),\n                    'timestamp': datetime.now().isoformat()\n                })\n            \n            self.is_running = False\n            raise\n    \n    async def resume(self, session_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Resume a previously interrupted campaign.\n        \n        Args:\n            session_id: Session ID to resume\n            \n        Returns:\n            Campaign results dictionary\n        \"\"\"\n        try:\n            self.logger.info(f\"Resuming campaign session: {session_id}\")\n            \n            # Check if session can be resumed\n            if not self.resume_manager.can_resume(session_id):\n                raise ValueError(f\"Session {session_id} cannot be resumed\")\n            \n            # Load session and checkpoint\n            session_info = self.resume_manager.load_session(session_id)\n            checkpoint = self.resume_manager.load_latest_checkpoint(session_id)\n            \n            if not session_info or not checkpoint:\n                raise ValueError(f\"Unable to load session data for {session_id}\")\n            \n            # Initialize campaign state\n            self.current_session_id = session_id\n            self.is_running = True\n            self.cancellation_requested = False\n            \n            # Resume session\n            self.resume_manager.start_session(session_id)\n            \n            # Determine resume point\n            stage = checkpoint.data.get('stage', 'unknown')\n            self.logger.info(f\"Resuming from stage: {stage}\")\n            \n            # Continue based on checkpoint stage\n            if stage == 'query_planning_complete':\n                query_plan = checkpoint.data['query_plan']\n                processed_config = checkpoint.data['processed_config']\n                \n                # Resume from agency initialization\n                agency = await self._initialize_agency(processed_config)\n                leads_results = await self._execute_query_plan(\n                    query_plan, agency, processed_config\n                )\n                \n            elif stage == 'queries_complete':\n                leads_results = checkpoint.data['leads_results']\n                processed_config = checkpoint.data['processed_config']\n                \n            elif stage == 'processing_complete':\n                validated_leads = checkpoint.data['validated_leads']\n                processed_config = checkpoint.data['processed_config']\n                \n                # Resume from export\n                output_files = await self._export_results(validated_leads, processed_config)\n                \n                return {\n                    'leads': validated_leads,\n                    'total_leads': len(validated_leads),\n                    'output_files': output_files,\n                    'resumed': True\n                }\n            else:\n                raise ValueError(f\"Unknown resume stage: {stage}\")\n            \n            # Continue with normal flow\n            validated_leads = await self._process_leads(leads_results, processed_config)\n            output_files = await self._export_results(validated_leads, processed_config)\n            \n            results_dict = {\n                'leads': validated_leads,\n                'total_leads': len(validated_leads),\n                'output_files': output_files,\n                'resumed': True\n            }\n            \n            self.resume_manager.complete_session(session_id, results_dict)\n            \n            self.logger.info(f\"Campaign resumed and completed: {len(validated_leads)} leads\")\n            return results_dict\n            \n        except Exception as e:\n            self.logger.error(f\"Campaign resume failed: {e}\", exc_info=True)\n            \n            if self.current_session_id:\n                self.resume_manager.fail_session(self.current_session_id, {\n                    'error': str(e),\n                    'timestamp': datetime.now().isoformat(),\n                    'stage': 'resume'\n                })\n            \n            self.is_running = False\n            raise\n    \n    def cancel(self):\n        \"\"\"Cancel the currently running campaign\"\"\"\n        self.logger.info(\"Campaign cancellation requested\")\n        self.cancellation_requested = True\n        \n        if self.current_session_id:\n            self.resume_manager.pause_session(self.current_session_id)\n    \n    def add_progress_callback(self, callback: Callable[[Dict[str, Any]], None]):\n        \"\"\"Add a progress callback function\"\"\"\n        self.progress_callbacks.append(callback)\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get current campaign status\"\"\"\n        status = {\n            'is_running': self.is_running,\n            'session_id': self.current_session_id,\n            'cancellation_requested': self.cancellation_requested,\n            'total_queries': self.total_queries,\n            'completed_queries': self.completed_queries,\n            'progress_percent': (self.completed_queries / self.total_queries * 100) if self.total_queries > 0 else 0\n        }\n        \n        if self.current_results:\n            status.update({\n                'campaign_name': self.current_results.campaign_name,\n                'start_time': self.current_results.start_time.isoformat(),\n                'total_leads': self.current_results.total_leads,\n                'successful_queries': self.current_results.successful_queries,\n                'failed_queries': self.current_results.failed_queries,\n                'errors_count': len(self.current_results.errors),\n                'warnings_count': len(self.current_results.warnings)\n            })\n        \n        return status\n    \n    # Private methods\n    \n    async def _validate_campaign_config(self, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate campaign configuration\"\"\"\n        errors = []\n        warnings = []\n        \n        # Required fields\n        required_fields = ['name', 'type']\n        for field in required_fields:\n            if field not in config:\n                errors.append(f\"Missing required field: {field}\")\n        \n        # Validate queries\n        if 'queries' not in config or not config['queries']:\n            errors.append(\"No search queries defined\")\n        elif len(config['queries']) > 50:\n            warnings.append(f\"Large number of queries ({len(config['queries'])}) may impact performance\")\n        \n        # Validate regions\n        if 'regions' not in config or not config['regions']:\n            warnings.append(\"No target regions specified, using defaults\")\n        \n        # Validate target leads\n        target_leads = config.get('target_leads', 100)\n        if target_leads > 1000:\n            warnings.append(f\"Large target ({target_leads} leads) may take significant time\")\n        \n        return {\n            'valid': len(errors) == 0,\n            'errors': errors,\n            'warnings': warnings\n        }\n    \n    async def _preprocess_campaign(self, config: Dict[str, Any], max_leads: Optional[int]) -> Dict[str, Any]:\n        \"\"\"Preprocess campaign configuration\"\"\"\n        processed = config.copy()\n        \n        # Apply max leads override\n        if max_leads:\n            processed['target_leads'] = min(processed.get('target_leads', 100), max_leads)\n        \n        # Set defaults\n        processed.setdefault('regions', ['United States'])\n        processed.setdefault('target_leads', 100)\n        processed.setdefault('quality_threshold', 0.7)\n        \n        # Expand regions if needed\n        if processed.get('expand_regions', False):\n            # Add logic for region expansion\n            pass\n        \n        # Process query templates\n        if 'query_templates' in processed:\n            expanded_queries = []\n            for template in processed['query_templates']:\n                for region in processed['regions']:\n                    expanded_queries.append(template.format(region=region))\n            processed['queries'] = expanded_queries\n        \n        self.logger.info(f\"Preprocessed campaign: {len(processed['queries'])} queries, \"\n                        f\"{len(processed['regions'])} regions, {processed['target_leads']} target leads\")\n        \n        return processed\n    \n    async def _generate_query_plan(self, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate optimized query execution plan\"\"\"\n        queries = config['queries']\n        regions = config['regions']\n        target_leads = config['target_leads']\n        \n        # Calculate leads per query\n        leads_per_query = max(1, target_leads // len(queries))\n        \n        # Generate query plan with optimization\n        query_plan = {\n            'queries': [],\n            'total_estimated_leads': 0,\n            'estimated_duration_minutes': 0,\n            'optimization_strategy': 'balanced'\n        }\n        \n        for i, query in enumerate(queries):\n            for region in regions:\n                query_info = {\n                    'id': f\"query_{i}_{region.lower().replace(' ', '_')}\",\n                    'query': query,\n                    'region': region,\n                    'target_leads': leads_per_query,\n                    'priority': 'normal',\n                    'estimated_duration_minutes': 5,\n                    'retry_count': 0,\n                    'max_retries': 3\n                }\n                \n                query_plan['queries'].append(query_info)\n                query_plan['total_estimated_leads'] += leads_per_query\n                query_plan['estimated_duration_minutes'] += 5\n        \n        # Optimize query order (prioritize high-success queries)\n        # This would be based on historical data in a real implementation\n        \n        return query_plan\n    \n    async def _initialize_agency(self, config: Dict[str, Any]) -> Any:\n        \"\"\"Initialize agency swarm for campaign\"\"\"\n        try:\n            # Create agency with campaign-specific configuration\n            agency_config = {\n                'campaign_type': config['type'],\n                'target_leads': config['target_leads'],\n                'quality_threshold': config.get('quality_threshold', 0.7),\n                'regions': config['regions']\n            }\n            \n            agency = await self.agency_factory.create_campaign_agency(agency_config)\n            \n            self.logger.info(\"Agency swarm initialized successfully\")\n            return agency\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize agency: {e}\")\n            raise\n    \n    async def _execute_query_plan(self, query_plan: Dict[str, Any], \n                                 agency: Any, config: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Execute query plan with agency coordination\"\"\"\n        all_leads = []\n        \n        try:\n            queries = query_plan['queries']\n            \n            # Execute queries with concurrency control\n            max_concurrent = self.config.search.max_concurrent_searches\n            semaphore = asyncio.Semaphore(max_concurrent)\n            \n            async def execute_single_query(query_info):\n                async with semaphore:\n                    if self.cancellation_requested:\n                        return []\n                    \n                    try:\n                        # Execute query through agency\n                        leads = await self._execute_single_query(query_info, agency)\n                        \n                        self.completed_queries += 1\n                        self.current_results.successful_queries += 1\n                        \n                        # Update progress\n                        progress = (self.completed_queries / self.total_queries) * 80 + 20  # 20-100%\n                        self.resume_manager.update_progress(\n                            progress=progress,\n                            current_step=f\"Completed query: {query_info['id']}\",\n                            leads_count=len(all_leads)\n                        )\n                        \n                        # Notify progress callbacks\n                        for callback in self.progress_callbacks:\n                            try:\n                                callback({\n                                    'progress': progress,\n                                    'completed_queries': self.completed_queries,\n                                    'total_queries': self.total_queries,\n                                    'leads_count': len(all_leads)\n                                })\n                            except Exception as e:\n                                self.logger.warning(f\"Progress callback error: {e}\")\n                        \n                        return leads\n                        \n                    except Exception as e:\n                        self.logger.error(f\"Query execution failed: {query_info['id']} - {e}\")\n                        self.current_results.failed_queries += 1\n                        self.current_results.errors.append({\n                            'timestamp': datetime.now().isoformat(),\n                            'query_id': query_info['id'],\n                            'error': str(e)\n                        })\n                        return []\n            \n            # Execute all queries concurrently\n            tasks = [execute_single_query(query_info) for query_info in queries]\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            \n            # Collect results\n            for result in results:\n                if isinstance(result, list):\n                    all_leads.extend(result)\n                elif isinstance(result, Exception):\n                    self.logger.error(f\"Query task failed: {result}\")\n            \n            # Save checkpoint\n            self.resume_manager.save_checkpoint({\n                'stage': 'queries_complete',\n                'leads_results': all_leads,\n                'processed_config': config\n            }, {\n                'total_progress': 80.0,\n                'current_step': f'Query execution complete - {len(all_leads)} leads found'\n            })\n            \n            self.logger.info(f\"Query execution completed: {len(all_leads)} leads found\")\n            return all_leads\n            \n        except Exception as e:\n            self.logger.error(f\"Query plan execution failed: {e}\")\n            raise\n    \n    async def _execute_single_query(self, query_info: Dict[str, Any], agency: Any) -> List[Dict[str, Any]]:\n        \"\"\"Execute a single query through the agency\"\"\"\n        # This would interface with the actual agency implementation\n        # For now, return mock data\n        await asyncio.sleep(1)  # Simulate processing time\n        \n        # Mock lead data\n        mock_leads = [\n            {\n                'name': f\"Business {i}\",\n                'email': f\"contact{i}@business.com\",\n                'phone': f\"555-{1000 + i}\",\n                'address': f\"{100 + i} Main St, {query_info['region']}\",\n                'website': f\"https://business{i}.com\",\n                'query_id': query_info['id'],\n                'source': 'mock_data'\n            }\n            for i in range(5)  # Mock 5 leads per query\n        ]\n        \n        return mock_leads\n    \n    async def _process_leads(self, leads: List[Dict[str, Any]], \n                           config: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Process and validate leads\"\"\"\n        try:\n            self.logger.info(f\"Processing {len(leads)} leads\")\n            \n            # Remove duplicates\n            unique_leads = self._deduplicate_leads(leads)\n            \n            # Validate leads\n            validated_leads = await self._validate_leads(unique_leads, config)\n            \n            # Score and filter leads\n            scored_leads = self._score_leads(validated_leads, config)\n            \n            # Apply quality threshold\n            quality_threshold = config.get('quality_threshold', 0.7)\n            filtered_leads = [lead for lead in scored_leads \n                            if lead.get('quality_score', 0) >= quality_threshold]\n            \n            # Limit to target count\n            target_leads = config.get('target_leads', 100)\n            final_leads = sorted(filtered_leads, \n                               key=lambda x: x.get('quality_score', 0), \n                               reverse=True)[:target_leads]\n            \n            # Save checkpoint\n            self.resume_manager.save_checkpoint({\n                'stage': 'processing_complete',\n                'validated_leads': final_leads,\n                'processed_config': config\n            }, {\n                'total_progress': 90.0,\n                'current_step': f'Lead processing complete - {len(final_leads)} valid leads'\n            })\n            \n            self.logger.info(f\"Lead processing completed: {len(final_leads)} leads\")\n            return final_leads\n            \n        except Exception as e:\n            self.logger.error(f\"Lead processing failed: {e}\")\n            raise\n    \n    def _deduplicate_leads(self, leads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Remove duplicate leads\"\"\"\n        seen_emails = set()\n        unique_leads = []\n        \n        for lead in leads:\n            email = lead.get('email', '').lower()\n            if email and email not in seen_emails:\n                seen_emails.add(email)\n                unique_leads.append(lead)\n        \n        return unique_leads\n    \n    async def _validate_leads(self, leads: List[Dict[str, Any]], \n                            config: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Validate leads data\"\"\"\n        validated = []\n        \n        for lead in leads:\n            # Basic validation\n            if self._is_valid_lead(lead):\n                validated.append(lead)\n        \n        return validated\n    \n    def _is_valid_lead(self, lead: Dict[str, Any]) -> bool:\n        \"\"\"Check if lead has minimum required data\"\"\"\n        required_fields = ['name', 'email']\n        return all(lead.get(field) for field in required_fields)\n    \n    def _score_leads(self, leads: List[Dict[str, Any]], \n                    config: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Score leads based on quality factors\"\"\"\n        for lead in leads:\n            score = 0.0\n            \n            # Email validation\n            if lead.get('email') and '@' in lead['email']:\n                score += 0.3\n            \n            # Phone number\n            if lead.get('phone'):\n                score += 0.2\n            \n            # Website\n            if lead.get('website'):\n                score += 0.2\n            \n            # Address\n            if lead.get('address'):\n                score += 0.1\n            \n            # Complete profile\n            if all(lead.get(field) for field in ['name', 'email', 'phone', 'address']):\n                score += 0.2\n            \n            lead['quality_score'] = min(1.0, score)\n        \n        return leads\n    \n    async def _export_results(self, leads: List[Dict[str, Any]], \n                            config: Dict[str, Any]) -> List[str]:\n        \"\"\"Export results to files\"\"\"\n        try:\n            output_files = []\n            \n            # Generate timestamp\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            campaign_name = config['name'].lower().replace(' ', '_')\n            \n            # CSV export\n            csv_filename = f\"{campaign_name}_leads_{timestamp}.csv\"\n            csv_path = Path(self.config.export.output_directory) / csv_filename\n            \n            # Export to CSV (simplified implementation)\n            import csv\n            \n            with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n                if leads:\n                    writer = csv.DictWriter(f, fieldnames=leads[0].keys())\n                    writer.writeheader()\n                    writer.writerows(leads)\n            \n            output_files.append(str(csv_path))\n            \n            # JSON export\n            json_filename = f\"{campaign_name}_leads_{timestamp}.json\"\n            json_path = Path(self.config.export.output_directory) / json_filename\n            \n            with open(json_path, 'w', encoding='utf-8') as f:\n                json.dump({\n                    'campaign': config['name'],\n                    'generated_at': datetime.now().isoformat(),\n                    'total_leads': len(leads),\n                    'leads': leads\n                }, f, indent=2, default=str)\n            \n            output_files.append(str(json_path))\n            \n            self.logger.info(f\"Results exported to {len(output_files)} files\")\n            return output_files\n            \n        except Exception as e:\n            self.logger.error(f\"Result export failed: {e}\")\n            raise"