"""
Performance Monitoring and Metrics Collection for VRSEN PubScrape

Provides comprehensive performance tracking, resource monitoring,
and metrics collection for optimization and debugging.
"""

import time
import psutil
import asyncio
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, field, asdict
from pathlib import Path
import json
import logging
from collections import deque, defaultdict
import statistics


@dataclass
class MetricPoint:
    """Single metric data point"""
    timestamp: datetime
    value: float
    tags: Dict[str, str] = field(default_factory=dict)


@dataclass
class PerformanceSnapshot:
    """Performance snapshot at a point in time"""
    timestamp: datetime
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    open_files: int
    threads_count: int
    active_operations: int = 0
    custom_metrics: Dict[str, float] = field(default_factory=dict)


@dataclass
class OperationMetrics:
    """Metrics for a specific operation"""
    operation_name: str
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    success: bool = True
    error_message: Optional[str] = None
    input_size: Optional[int] = None
    output_size: Optional[int] = None
    memory_peak_mb: Optional[float] = None
    cpu_time_seconds: Optional[float] = None
    custom_data: Dict[str, Any] = field(default_factory=dict)


class PerformanceMonitor:
    """
    Comprehensive performance monitoring system.
    
    Features:
    - Real-time system resource monitoring
    - Operation-level performance tracking
    - Custom metrics collection
    - Trend analysis and alerting
    - Performance reporting and visualization
    - Resource usage optimization suggestions
    """
    
    def __init__(self, 
                 output_dir: str = "output",
                 collection_interval: float = 5.0,  # seconds
                 history_size: int = 1000,
                 enable_alerts: bool = True,
                 logger: Optional[logging.Logger] = None):
        
        self.output_dir = Path(output_dir)
        self.collection_interval = collection_interval
        self.history_size = history_size
        self.enable_alerts = enable_alerts
        self.logger = logger or logging.getLogger(__name__)
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.metrics_dir = self.output_dir / "metrics"
        self.metrics_dir.mkdir(exist_ok=True)
        
        # Monitoring state
        self.is_monitoring = False
        self.monitor_task: Optional[asyncio.Task] = None
        self.start_time: Optional[datetime] = None
        
        # Data storage
        self.performance_history: deque = deque(maxlen=history_size)
        self.operation_metrics: Dict[str, OperationMetrics] = {}\n        self.custom_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=history_size))\n        self.alert_thresholds = self._default_alert_thresholds()\n        \n        # Process monitoring\n        self.process = psutil.Process()\n        self.baseline_stats = None\n        \n        # Lock for thread safety\n        self._lock = threading.Lock()\n        \n        self.logger.info(f\"Performance monitor initialized: {self.output_dir}\")\n    \n    def _default_alert_thresholds(self) -> Dict[str, float]:\n        \"\"\"Default alert thresholds\"\"\"\n        return {\n            \"cpu_percent\": 80.0,\n            \"memory_percent\": 85.0,\n            \"disk_io_rate_mb_s\": 100.0,\n            \"operation_duration_seconds\": 300.0,  # 5 minutes\n            \"error_rate_percent\": 5.0\n        }\n    \n    def start(self) -> bool:\n        \"\"\"\n        Start performance monitoring.\n        \n        Returns:\n            True if started successfully\n        \"\"\"\n        try:\n            if self.is_monitoring:\n                self.logger.warning(\"Performance monitoring already running\")\n                return True\n            \n            self.start_time = datetime.now()\n            self.baseline_stats = self._collect_baseline_stats()\n            self.is_monitoring = True\n            \n            # Start monitoring task\n            if asyncio.get_event_loop().is_running():\n                self.monitor_task = asyncio.create_task(self._monitoring_loop())\n            else:\n                # Start in separate thread if no event loop\n                thread = threading.Thread(target=self._monitoring_thread, daemon=True)\n                thread.start()\n            \n            self.logger.info(\"Performance monitoring started\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to start performance monitoring: {e}\")\n            return False\n    \n    def stop(self) -> bool:\n        \"\"\"\n        Stop performance monitoring.\n        \n        Returns:\n            True if stopped successfully\n        \"\"\"\n        try:\n            if not self.is_monitoring:\n                return True\n            \n            self.is_monitoring = False\n            \n            # Cancel monitoring task\n            if self.monitor_task and not self.monitor_task.done():\n                self.monitor_task.cancel()\n            \n            # Save final metrics\n            self._save_metrics_to_file()\n            \n            self.logger.info(\"Performance monitoring stopped\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to stop performance monitoring: {e}\")\n            return False\n    \n    def start_operation(self, operation_name: str, **kwargs) -> str:\n        \"\"\"\n        Start tracking an operation.\n        \n        Args:\n            operation_name: Name of the operation\n            **kwargs: Additional operation data\n            \n        Returns:\n            Operation ID for tracking\n        \"\"\"\n        operation_id = f\"{operation_name}_{int(time.time() * 1000)}\"\n        \n        with self._lock:\n            self.operation_metrics[operation_id] = OperationMetrics(\n                operation_name=operation_name,\n                start_time=datetime.now(),\n                custom_data=kwargs\n            )\n        \n        self.logger.debug(f\"Started operation: {operation_name} ({operation_id})\")\n        return operation_id\n    \n    def end_operation(self, operation_id: str, success: bool = True, \n                     error_message: Optional[str] = None, **kwargs) -> Optional[OperationMetrics]:\n        \"\"\"\n        End tracking an operation.\n        \n        Args:\n            operation_id: Operation ID from start_operation\n            success: Whether operation succeeded\n            error_message: Error message if failed\n            **kwargs: Additional operation data\n            \n        Returns:\n            OperationMetrics object or None if not found\n        \"\"\"\n        with self._lock:\n            if operation_id not in self.operation_metrics:\n                self.logger.warning(f\"Operation not found: {operation_id}\")\n                return None\n            \n            metrics = self.operation_metrics[operation_id]\n            metrics.end_time = datetime.now()\n            metrics.duration_seconds = (metrics.end_time - metrics.start_time).total_seconds()\n            metrics.success = success\n            metrics.error_message = error_message\n            metrics.custom_data.update(kwargs)\n            \n            # Check for performance alerts\n            if self.enable_alerts:\n                self._check_operation_alerts(metrics)\n        \n        self.logger.debug(f\"Ended operation: {metrics.operation_name} \"\n                         f\"({operation_id}) - {metrics.duration_seconds:.2f}s\")\n        \n        return metrics\n    \n    def record_metric(self, metric_name: str, value: float, **tags):\n        \"\"\"\n        Record a custom metric.\n        \n        Args:\n            metric_name: Name of the metric\n            value: Metric value\n            **tags: Additional tags for the metric\n        \"\"\"\n        metric_point = MetricPoint(\n            timestamp=datetime.now(),\n            value=value,\n            tags=tags\n        )\n        \n        with self._lock:\n            self.custom_metrics[metric_name].append(metric_point)\n        \n        self.logger.debug(f\"Recorded metric: {metric_name} = {value}\")\n    \n    def get_current_stats(self) -> PerformanceSnapshot:\n        \"\"\"\n        Get current performance statistics.\n        \n        Returns:\n            Current PerformanceSnapshot\n        \"\"\"\n        try:\n            # Get system stats\n            cpu_percent = self.process.cpu_percent()\n            memory_info = self.process.memory_info()\n            memory_mb = memory_info.rss / 1024 / 1024\n            memory_percent = self.process.memory_percent()\n            \n            # Get I/O stats\n            io_counters = self.process.io_counters()\n            disk_read_mb = io_counters.read_bytes / 1024 / 1024\n            disk_write_mb = io_counters.write_bytes / 1024 / 1024\n            \n            # Get network stats (system-wide)\n            net_io = psutil.net_io_counters()\n            net_sent_mb = net_io.bytes_sent / 1024 / 1024\n            net_recv_mb = net_io.bytes_recv / 1024 / 1024\n            \n            # Get process info\n            open_files = len(self.process.open_files())\n            threads_count = self.process.num_threads()\n            \n            # Active operations\n            with self._lock:\n                active_ops = len([m for m in self.operation_metrics.values() if m.end_time is None])\n            \n            return PerformanceSnapshot(\n                timestamp=datetime.now(),\n                cpu_percent=cpu_percent,\n                memory_mb=memory_mb,\n                memory_percent=memory_percent,\n                disk_io_read_mb=disk_read_mb,\n                disk_io_write_mb=disk_write_mb,\n                network_sent_mb=net_sent_mb,\n                network_recv_mb=net_recv_mb,\n                open_files=open_files,\n                threads_count=threads_count,\n                active_operations=active_ops\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get current stats: {e}\")\n            return PerformanceSnapshot(\n                timestamp=datetime.now(),\n                cpu_percent=0.0,\n                memory_mb=0.0,\n                memory_percent=0.0,\n                disk_io_read_mb=0.0,\n                disk_io_write_mb=0.0,\n                network_sent_mb=0.0,\n                network_recv_mb=0.0,\n                open_files=0,\n                threads_count=0\n            )\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n        \"\"\"\n        Get comprehensive metrics summary.\n        \n        Returns:\n            Metrics summary dictionary\n        \"\"\"\n        try:\n            current_stats = self.get_current_stats()\n            \n            with self._lock:\n                # Operation metrics summary\n                completed_ops = [m for m in self.operation_metrics.values() if m.end_time is not None]\n                running_ops = [m for m in self.operation_metrics.values() if m.end_time is None]\n                \n                successful_ops = [m for m in completed_ops if m.success]\n                failed_ops = [m for m in completed_ops if not m.success]\n                \n                # Performance history stats\n                if self.performance_history:\n                    cpu_values = [s.cpu_percent for s in self.performance_history]\n                    memory_values = [s.memory_mb for s in self.performance_history]\n                    \n                    cpu_avg = statistics.mean(cpu_values)\n                    cpu_max = max(cpu_values)\n                    memory_avg = statistics.mean(memory_values)\n                    memory_max = max(memory_values)\n                else:\n                    cpu_avg = cpu_max = current_stats.cpu_percent\n                    memory_avg = memory_max = current_stats.memory_mb\n            \n            # Runtime calculation\n            runtime = datetime.now() - self.start_time if self.start_time else timedelta(0)\n            \n            summary = {\n                \"monitoring_status\": \"active\" if self.is_monitoring else \"stopped\",\n                \"runtime_seconds\": runtime.total_seconds(),\n                \"current_stats\": asdict(current_stats),\n                \"resource_usage\": {\n                    \"cpu_percent_avg\": cpu_avg,\n                    \"cpu_percent_max\": cpu_max,\n                    \"cpu_percent_current\": current_stats.cpu_percent,\n                    \"memory_mb_avg\": memory_avg,\n                    \"memory_mb_max\": memory_max,\n                    \"memory_mb_current\": current_stats.memory_mb,\n                    \"memory_percent_current\": current_stats.memory_percent\n                },\n                \"operations\": {\n                    \"total_completed\": len(completed_ops),\n                    \"total_running\": len(running_ops),\n                    \"successful\": len(successful_ops),\n                    \"failed\": len(failed_ops),\n                    \"success_rate\": len(successful_ops) / len(completed_ops) * 100 if completed_ops else 0,\n                    \"avg_duration_seconds\": statistics.mean([m.duration_seconds for m in completed_ops if m.duration_seconds]) if completed_ops else 0\n                },\n                \"alerts\": self._get_active_alerts(current_stats),\n                \"data_points_collected\": len(self.performance_history),\n                \"custom_metrics_count\": len(self.custom_metrics)\n            }\n            \n            # Add operation breakdown by type\n            operation_types = defaultdict(list)\n            for op in completed_ops:\n                operation_types[op.operation_name].append(op)\n            \n            operation_breakdown = {}\n            for op_type, ops in operation_types.items():\n                durations = [op.duration_seconds for op in ops if op.duration_seconds]\n                operation_breakdown[op_type] = {\n                    \"count\": len(ops),\n                    \"success_rate\": len([op for op in ops if op.success]) / len(ops) * 100,\n                    \"avg_duration_seconds\": statistics.mean(durations) if durations else 0,\n                    \"total_duration_seconds\": sum(durations) if durations else 0\n                }\n            \n            summary[\"operation_breakdown\"] = operation_breakdown\n            \n            return summary\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get metrics summary: {e}\")\n            return {\"error\": str(e)}\n    \n    def save_metrics_report(self, filename: Optional[str] = None) -> str:\n        \"\"\"\n        Save comprehensive metrics report to file.\n        \n        Args:\n            filename: Optional custom filename\n            \n        Returns:\n            Path to saved report file\n        \"\"\"\n        try:\n            if filename is None:\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                filename = f\"performance_report_{timestamp}.json\"\n            \n            report_path = self.metrics_dir / filename\n            \n            # Generate comprehensive report\n            report = {\n                \"generated_at\": datetime.now().isoformat(),\n                \"monitoring_period\": {\n                    \"start_time\": self.start_time.isoformat() if self.start_time else None,\n                    \"end_time\": datetime.now().isoformat(),\n                    \"duration_seconds\": (datetime.now() - self.start_time).total_seconds() if self.start_time else 0\n                },\n                \"summary\": self.get_metrics_summary(),\n                \"baseline_stats\": asdict(self.baseline_stats) if self.baseline_stats else None,\n                \"performance_history\": [asdict(s) for s in list(self.performance_history)],\n                \"operation_metrics\": {op_id: asdict(metrics) for op_id, metrics in self.operation_metrics.items()},\n                \"custom_metrics\": {name: [asdict(point) for point in list(points)] \n                                  for name, points in self.custom_metrics.items()},\n                \"alert_thresholds\": self.alert_thresholds,\n                \"recommendations\": self._generate_recommendations()\n            }\n            \n            # Convert datetime objects to strings\n            report = self._serialize_datetime_objects(report)\n            \n            with open(report_path, 'w') as f:\n                json.dump(report, f, indent=2, default=str)\n            \n            self.logger.info(f\"Performance report saved: {report_path}\")\n            return str(report_path)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save metrics report: {e}\")\n            return \"\"\n    \n    def optimize_performance(self) -> List[str]:\n        \"\"\"\n        Analyze performance and provide optimization suggestions.\n        \n        Returns:\n            List of optimization recommendations\n        \"\"\"\n        recommendations = []\n        \n        try:\n            current_stats = self.get_current_stats()\n            summary = self.get_metrics_summary()\n            \n            # CPU optimization\n            if current_stats.cpu_percent > 80:\n                recommendations.append(\n                    \"High CPU usage detected. Consider reducing concurrent operations or \"\n                    \"implementing more efficient algorithms.\"\n                )\n            \n            # Memory optimization\n            if current_stats.memory_percent > 85:\n                recommendations.append(\n                    \"High memory usage detected. Consider implementing data streaming, \"\n                    \"reducing batch sizes, or clearing unused objects.\"\n                )\n            \n            # Operation performance\n            ops_summary = summary.get(\"operations\", {})\n            if ops_summary.get(\"avg_duration_seconds\", 0) > 60:\n                recommendations.append(\n                    \"Operations are taking longer than expected. Consider optimizing \"\n                    \"algorithms, reducing data processing size, or implementing parallelization.\"\n                )\n            \n            # Error rate\n            if ops_summary.get(\"success_rate\", 100) < 95:\n                recommendations.append(\n                    \"High error rate detected. Review error logs and implement \"\n                    \"better error handling and retry mechanisms.\"\n                )\n            \n            # Resource scaling\n            if current_stats.threads_count > 50:\n                recommendations.append(\n                    \"High number of threads detected. Consider using async/await patterns \"\n                    \"or connection pooling to reduce thread overhead.\"\n                )\n            \n            if not recommendations:\n                recommendations.append(\"Performance appears optimal. No immediate optimizations needed.\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate optimization recommendations: {e}\")\n            recommendations.append(f\"Error analyzing performance: {e}\")\n        \n        return recommendations\n    \n    # Private methods\n    \n    async def _monitoring_loop(self):\n        \"\"\"Main monitoring loop (async version)\"\"\"\n        try:\n            while self.is_monitoring:\n                snapshot = self.get_current_stats()\n                \n                with self._lock:\n                    self.performance_history.append(snapshot)\n                \n                # Check for alerts\n                if self.enable_alerts:\n                    self._check_system_alerts(snapshot)\n                \n                await asyncio.sleep(self.collection_interval)\n                \n        except asyncio.CancelledError:\n            self.logger.info(\"Monitoring loop cancelled\")\n        except Exception as e:\n            self.logger.error(f\"Monitoring loop error: {e}\")\n    \n    def _monitoring_thread(self):\n        \"\"\"Main monitoring loop (thread version)\"\"\"\n        try:\n            while self.is_monitoring:\n                snapshot = self.get_current_stats()\n                \n                with self._lock:\n                    self.performance_history.append(snapshot)\n                \n                # Check for alerts\n                if self.enable_alerts:\n                    self._check_system_alerts(snapshot)\n                \n                time.sleep(self.collection_interval)\n                \n        except Exception as e:\n            self.logger.error(f\"Monitoring thread error: {e}\")\n    \n    def _collect_baseline_stats(self) -> PerformanceSnapshot:\n        \"\"\"Collect baseline performance statistics\"\"\"\n        # Collect a few samples to get a stable baseline\n        samples = []\n        for _ in range(5):\n            samples.append(self.get_current_stats())\n            time.sleep(0.5)\n        \n        # Calculate average baseline\n        avg_cpu = statistics.mean([s.cpu_percent for s in samples])\n        avg_memory = statistics.mean([s.memory_mb for s in samples])\n        avg_memory_pct = statistics.mean([s.memory_percent for s in samples])\n        \n        baseline = samples[0]  # Use first sample as template\n        baseline.cpu_percent = avg_cpu\n        baseline.memory_mb = avg_memory\n        baseline.memory_percent = avg_memory_pct\n        \n        return baseline\n    \n    def _check_system_alerts(self, snapshot: PerformanceSnapshot):\n        \"\"\"Check for system-level performance alerts\"\"\"\n        alerts = []\n        \n        if snapshot.cpu_percent > self.alert_thresholds[\"cpu_percent\"]:\n            alerts.append(f\"High CPU usage: {snapshot.cpu_percent:.1f}%\")\n        \n        if snapshot.memory_percent > self.alert_thresholds[\"memory_percent\"]:\n            alerts.append(f\"High memory usage: {snapshot.memory_percent:.1f}%\")\n        \n        for alert in alerts:\n            self.logger.warning(f\"Performance Alert: {alert}\")\n    \n    def _check_operation_alerts(self, metrics: OperationMetrics):\n        \"\"\"Check for operation-level performance alerts\"\"\"\n        if metrics.duration_seconds and metrics.duration_seconds > self.alert_thresholds[\"operation_duration_seconds\"]:\n            self.logger.warning(f\"Slow operation detected: {metrics.operation_name} \"\n                              f\"took {metrics.duration_seconds:.1f} seconds\")\n        \n        if not metrics.success:\n            self.logger.warning(f\"Operation failed: {metrics.operation_name} - {metrics.error_message}\")\n    \n    def _get_active_alerts(self, current_stats: PerformanceSnapshot) -> List[str]:\n        \"\"\"Get list of currently active alerts\"\"\"\n        alerts = []\n        \n        if current_stats.cpu_percent > self.alert_thresholds[\"cpu_percent\"]:\n            alerts.append(f\"High CPU usage: {current_stats.cpu_percent:.1f}%\")\n        \n        if current_stats.memory_percent > self.alert_thresholds[\"memory_percent\"]:\n            alerts.append(f\"High memory usage: {current_stats.memory_percent:.1f}%\")\n        \n        return alerts\n    \n    def _generate_recommendations(self) -> List[str]:\n        \"\"\"Generate performance recommendations based on collected data\"\"\"\n        return self.optimize_performance()\n    \n    def _save_metrics_to_file(self):\n        \"\"\"Save current metrics to file\"\"\"\n        try:\n            self.save_metrics_report()\n        except Exception as e:\n            self.logger.error(f\"Failed to save metrics to file: {e}\")\n    \n    def _serialize_datetime_objects(self, obj):\n        \"\"\"Recursively convert datetime objects to ISO strings\"\"\"\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, dict):\n            return {k: self._serialize_datetime_objects(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._serialize_datetime_objects(item) for item in obj]\n        else:\n            return obj"