# VRSEN PubScrape CLI Usage Guide\n\nComprehensive guide for using the VRSEN PubScrape command-line interface.\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Installation](#installation)\n- [Basic Usage](#basic-usage)\n- [Advanced Features](#advanced-features)\n- [Campaign Configuration](#campaign-configuration)\n- [Docker Deployment](#docker-deployment)\n- [Troubleshooting](#troubleshooting)\n- [API Reference](#api-reference)\n\n## Quick Start\n\n### 1. Basic Lead Generation\n\n```bash\n# Generate 50 leads with default settings\npython main.py scrape --campaign campaigns/doctors.yaml --max-leads 50\n\n# Quick restaurant leads in specific city\npython scripts/run_scraper.py --type restaurant --targets 25\n\n# Validate existing leads\npython main.py validate --input output/leads.csv --level strict\n```\n\n### 2. Using the Smart Runner\n\n```bash\n# Intelligent runner with auto-optimization\npython scripts/run_scraper.py --campaign campaigns/lawyers.yaml --targets 100\n\n# Debug mode with comprehensive logging\npython scripts/run_scraper.py --debug --verbose --type doctor\n\n# Resume interrupted campaign\npython scripts/run_scraper.py --resume campaign_20240101_120000\n```\n\n## Installation\n\n### Prerequisites\n\n- Python 3.11+\n- Chrome/Chromium browser\n- 4GB+ RAM recommended\n- 5GB+ disk space\n\n### Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/your-org/vrsen-pubscrape.git\ncd vrsen-pubscrape\n\n# Install dependencies\npip install -r requirements.txt\n\n# Copy environment configuration\ncp .env.example .env\n\n# Edit configuration\nnano .env  # Add your API keys\n```\n\n### Required API Keys\n\n```env\n# Essential\nOPENAI_API_KEY=your_openai_key_here\n\n# Recommended\nBING_API_KEY=your_bing_key_here\nHUNTER_API_KEY=your_hunter_key_here\nMAILTESTER_API_KEY=your_mailtester_key_here\n```\n\n## Basic Usage\n\n### Main CLI Interface\n\n```bash\npython main.py [command] [options]\n```\n\n#### Available Commands\n\n| Command | Description | Example |\n|---------|-------------|---------|\n| `scrape` | Run scraping campaign | `python main.py scrape --campaign campaigns/doctors.yaml` |\n| `validate` | Validate lead data | `python main.py validate --input leads.csv --level strict` |\n| `export` | Export to different formats | `python main.py export --input leads.csv --format xlsx` |\n| `status` | Show system status | `python main.py status --sessions` |\n| `config` | Manage configuration | `python main.py config --show` |\n\n### Smart Runner Script\n\n```bash\npython scripts/run_scraper.py [options]\n```\n\n#### Key Features\n\n- **Auto-optimization**: Adjusts settings based on system resources\n- **Pre-flight checks**: Validates environment before execution\n- **Campaign intelligence**: Provides estimates and recommendations\n- **Progress tracking**: Real-time progress monitoring\n\n## Advanced Features\n\n### Campaign Types\n\n#### Doctor Leads\n```bash\n# Medical professionals with strict validation\npython scripts/run_scraper.py --type doctor --targets 100 \\\n  --rate-limit 8 --validation strict\n```\n\n#### Lawyer Leads\n```bash\n# Legal professionals with high-quality validation\npython scripts/run_scraper.py --type lawyer --targets 200 \\\n  --validation strict --format xlsx\n```\n\n#### Restaurant Leads\n```bash\n# Local businesses with phone validation\npython scripts/run_scraper.py --type restaurant --targets 50 \\\n  --phone-validation enabled\n```\n\n### Resume Functionality\n\n```bash\n# List available sessions\npython main.py status --sessions\n\n# Resume specific session\npython main.py scrape --resume campaign_20240101_120000\n\n# Resume with modifications\npython scripts/run_scraper.py --resume campaign_20240101_120000 --targets 500\n```\n\n### Validation Levels\n\n| Level | Description | Use Case |\n|-------|-------------|----------|\n| `lenient` | Basic syntax checking | Quick validation |\n| `moderate` | DNS and format validation | Standard quality |\n| `strict` | Full deliverability checking | High-quality leads |\n\n```bash\n# Different validation levels\npython main.py validate --input leads.csv --level lenient\npython main.py validate --input leads.csv --level moderate  \npython main.py validate --input leads.csv --level strict\n```\n\n### Export Formats\n\n```bash\n# Export to Excel with metadata\npython main.py export --input leads.csv --format xlsx\n\n# Export to JSON without metadata\npython main.py export --input leads.csv --format json --no-metadata\n\n# Export with compression\npython main.py export --input leads.csv --format csv --compress\n```\n\n## Campaign Configuration\n\n### YAML Campaign File\n\n```yaml\n# campaigns/example.yaml\nname: \"Professional Services Campaign\"\ntype: \"doctor\"\ntarget_leads: 100\nregions:\n  - \"United States\"\n  - \"Canada\"\n\nqueries:\n  - \"doctor contact information {region}\"\n  - \"medical practice email {region}\"\n  - \"physician directory {region}\"\n\nfilters:\n  email_required: true\n  phone_preferred: true\n  quality_threshold: 0.7\n\nsettings:\n  rate_limit_rpm: 10\n  max_pages_per_query: 3\n  validation_level: \"strict\"\n```\n\n### Dynamic Configuration\n\n```bash\n# Override campaign settings\npython main.py scrape --campaign campaigns/doctors.yaml \\\n  --max-leads 50 \\\n  --rate-limit 15 \\\n  --max-pages 5\n```\n\n### Template Variables\n\nCampaigns support template variables:\n\n- `{region}` - Target region\n- `{type}` - Business type\n- `{city}` - Specific city\n- `{state}` - US state\n\n```yaml\nqueries:\n  - \"{type} in {city}, {state} contact information\"\n  - \"{type} {region} professional directory\"\n```\n\n## Docker Deployment\n\n### Development Setup\n\n```bash\n# Build and run development environment\ndocker-compose --profile development up -d\n\n# Run scraping campaign\ndocker-compose exec pubscrape-dev python main.py scrape --campaign campaigns/test.yaml\n```\n\n### Production Deployment\n\n```bash\n# Production setup with monitoring\ndocker-compose --profile production --profile monitoring up -d\n\n# Scale workers\ndocker-compose up --scale pubscrape=3\n```\n\n### Docker Commands\n\n```bash\n# View logs\ndocker-compose logs -f pubscrape\n\n# Execute commands in container\ndocker-compose exec pubscrape python scripts/run_scraper.py --help\n\n# Access database\ndocker-compose exec postgres psql -U pubscrape -d pubscrape\n\n# Monitor performance\ndocker-compose exec pubscrape python -c \"from src.utils.performance_monitor import PerformanceMonitor; print(PerformanceMonitor().get_metrics_summary())\"\n```\n\n## Troubleshooting\n\n### Common Issues\n\n#### 1. API Key Errors\n```bash\n# Check API key configuration\npython main.py config --show | grep -i api\n\n# Test API connectivity\npython -c \"import openai; print('OpenAI connection OK')\"\n```\n\n#### 2. Browser Issues\n```bash\n# Check Chrome installation\ngoogle-chrome --version\n\n# Run with visible browser (debug)\nexport HEADLESS_BROWSER=false\npython scripts/run_scraper.py --debug\n```\n\n#### 3. Permission Errors\n```bash\n# Fix output directory permissions\nsudo chown -R $USER:$USER output/ logs/ cache/\nchmod -R 755 output/ logs/ cache/\n```\n\n#### 4. Memory Issues\n```bash\n# Reduce batch size\nexport BATCH_SIZE=50\nexport MAX_CONCURRENT_SEARCHES=1\n\n# Monitor memory usage\npython scripts/run_scraper.py --monitor\n```\n\n### Debug Mode\n\n```bash\n# Maximum debugging\npython scripts/run_scraper.py --debug --verbose \\\n  --log-level DEBUG \\\n  --dry-run\n\n# Check system status\npython main.py status\n\n# Validate configuration\npython main.py config --validate\n```\n\n### Log Analysis\n\n```bash\n# View recent logs\ntail -f logs/vrsen-pubscrape.log\n\n# Search for errors\ngrep -i error logs/vrsen-pubscrape.log\n\n# Performance logs\ntail -f logs/vrsen-pubscrape_performance.log\n```\n\n## API Reference\n\n### Command Line Options\n\n#### Global Options\n| Option | Description | Default |\n|--------|-------------|----------|\n| `--config` | Configuration file | Auto-detect |\n| `--verbose` | Enable verbose logging | False |\n| `--quiet` | Quiet mode | False |\n| `--debug` | Debug mode | False |\n| `--output-dir` | Output directory | `output` |\n| `--dry-run` | Simulate without execution | False |\n\n#### Scrape Command\n| Option | Description | Default |\n|--------|-------------|----------|\n| `--campaign` | Campaign config file | Required |\n| `--max-leads` | Maximum leads to generate | Campaign setting |\n| `--max-pages` | Maximum pages per query | 5 |\n| `--rate-limit` | Requests per minute | 12 |\n| `--resume` | Resume session ID | None |\n| `--no-validation` | Skip validation | False |\n\n#### Validate Command\n| Option | Description | Default |\n|--------|-------------|----------|\n| `--input` | Input CSV file | Required |\n| `--level` | Validation level | `strict` |\n| `--output-dir` | Output directory | `output` |\n\n#### Export Command\n| Option | Description | Default |\n|--------|-------------|----------|\n| `--input` | Input file | Required |\n| `--format` | Output format | `csv` |\n| `--output-dir` | Output directory | `output` |\n| `--no-metadata` | Exclude metadata | False |\n\n### Environment Variables\n\n#### Essential Variables\n```env\nVRSEN_ENV=development|staging|production\nOPENAI_API_KEY=your_key_here\nLOG_LEVEL=INFO|DEBUG|WARNING|ERROR\n```\n\n#### Performance Tuning\n```env\nMAX_PAGES_PER_QUERY=5\nRATE_LIMIT_RPM=12\nMAX_CONCURRENT_SEARCHES=2\nBATCH_SIZE=100\nMAX_WORKERS=4\n```\n\n#### Output Configuration\n```env\nOUTPUT_DIR=output\nDATA_DIR=data\nCACHE_DIR=cache\nCOMPRESS_OUTPUT=false\nMAX_FILE_SIZE_MB=100\n```\n\n### Exit Codes\n\n| Code | Meaning |\n|------|----------|\n| 0 | Success |\n| 1 | General error |\n| 2 | Configuration error |\n| 3 | API error |\n| 4 | Validation error |\n| 130 | Interrupted by user |\n\n### File Locations\n\n```\npubscrape/\n├── campaigns/          # Campaign configurations\n├── output/            # Generated leads and reports\n├── logs/              # Application logs\n├── cache/             # Cached data\n├── data/              # Internal data storage\n├── state/             # Session state for resume\n├── configs/           # Configuration files\n└── scripts/           # Utility scripts\n```\n\n### Performance Monitoring\n\n```bash\n# Real-time monitoring\npython -c \"\nfrom src.utils.performance_monitor import PerformanceMonitor\nmonitor = PerformanceMonitor()\nmonitor.start()\nprint('Monitoring started - check logs/')\n\"\n\n# Generate performance report\npython -c \"\nfrom src.utils.performance_monitor import PerformanceMonitor\nmonitor = PerformanceMonitor()\nreport_path = monitor.save_metrics_report()\nprint(f'Report saved: {report_path}')\n\"\n```\n\n### Integration Examples\n\n#### Python Script Integration\n```python\n#!/usr/bin/env python3\nimport asyncio\nfrom src.pipeline.campaign_runner import CampaignRunner\nfrom src.config.config_manager import config_manager\n\nasync def run_campaign():\n    config = {\n        'name': 'API Campaign',\n        'type': 'doctor',\n        'target_leads': 50,\n        'queries': ['doctor contact info'],\n        'regions': ['California']\n    }\n    \n    runner = CampaignRunner(\n        config=config_manager.config,\n        # ... other dependencies\n    )\n    \n    results = await runner.run(config, 'api_session_001')\n    print(f\"Generated {len(results['leads'])} leads\")\n\nif __name__ == '__main__':\n    asyncio.run(run_campaign())\n```\n\n#### Bash Script Integration\n```bash\n#!/bin/bash\n\n# Automated lead generation pipeline\nSESSION_ID=\"batch_$(date +%Y%m%d_%H%M%S)\"\n\necho \"Starting lead generation session: $SESSION_ID\"\n\n# Run campaign\npython scripts/run_scraper.py \\\n  --campaign campaigns/doctors.yaml \\\n  --targets 100 \\\n  --output-dir \"output/$SESSION_ID\" \\\n  --session-id \"$SESSION_ID\"\n\n# Validate results\npython main.py validate \\\n  --input \"output/$SESSION_ID/leads.csv\" \\\n  --level strict \\\n  --output-dir \"output/$SESSION_ID/validated\"\n\n# Export to multiple formats\nfor format in csv xlsx json; do\n  python main.py export \\\n    --input \"output/$SESSION_ID/validated/leads_valid.csv\" \\\n    --format \"$format\" \\\n    --output-dir \"output/$SESSION_ID/final\"\ndone\n\necho \"Pipeline completed: output/$SESSION_ID/final/\"\n```\n\n## Best Practices\n\n### 1. Resource Management\n- Monitor system resources during large campaigns\n- Use appropriate batch sizes for your system\n- Enable caching for repeated operations\n- Clean up old sessions regularly\n\n### 2. Quality Control\n- Always validate leads before use\n- Use strict validation for high-value campaigns\n- Review error logs for patterns\n- Test campaigns with small samples first\n\n### 3. Rate Limiting\n- Respect target website rate limits\n- Use appropriate delays between requests\n- Monitor for rate limit errors\n- Adjust concurrency based on target\n\n### 4. Data Management\n- Organize campaigns by type and date\n- Use descriptive naming conventions\n- Backup important campaign results\n- Archive old data regularly\n\n### 5. Security\n- Store API keys securely\n- Use environment-specific configurations\n- Monitor access logs\n- Rotate API keys regularly\n\n---\n\n**Need Help?** \n\nFor additional support:\n- Check the [GitHub Issues](https://github.com/your-org/vrsen-pubscrape/issues)\n- Review the [API Documentation](API_REFERENCE.md)\n- See [Docker Guide](DOCKER_DEPLOYMENT.md) for containerization\n- Contact support: support@vrsen.com"