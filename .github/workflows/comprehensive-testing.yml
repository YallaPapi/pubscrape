name: Comprehensive Testing Suite

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - performance
          - security
          - smoke

env:
  PYTHONPATH: ${{ github.workspace }}
  PYTEST_ADDOPTS: --tb=short --strict-markers

jobs:
  # Job 1: Code Quality and Static Analysis
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Run security analysis with Bandit
      run: bandit -r src/ -f json -o reports/bandit-report.json || true
    
    - name: Check dependencies for vulnerabilities
      run: safety check --json --output reports/safety-report.json || true
    
    - name: Run linting
      run: |
        flake8 src/ tests/ --output-file reports/flake8-report.txt || true
        mypy src/ --no-error-summary --json-report reports/mypy-report.json || true
    
    - name: Upload code quality reports
      uses: actions/upload-artifact@v3
      with:
        name: code-quality-reports
        path: reports/

  # Job 2: Unit Tests with Coverage
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Create test reports directory
      run: mkdir -p tests/reports
    
    - name: Run unit tests with coverage
      run: |
        pytest tests/unit/ \
          --cov=src \
          --cov-report=term \
          --cov-report=xml:tests/reports/coverage.xml \
          --cov-report=html:tests/reports/coverage_html \
          --cov-report=json:tests/reports/coverage.json \
          --html=tests/reports/unit-test-report.html \
          --self-contained-html \
          --json-report --json-report-file=tests/reports/unit-report.json \
          --benchmark-json=tests/reports/unit-benchmarks.json \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: tests/reports/coverage.xml
        name: unit-tests-${{ matrix.python-version }}
        fail_ci_if_error: false
    
    - name: Upload unit test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-reports-${{ matrix.python-version }}
        path: tests/reports/

  # Job 3: Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          chromium-browser \
          chromium-chromedriver \
          xvfb
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-integration-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-integration-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Create test reports directory
      run: mkdir -p tests/reports
    
    - name: Run integration tests
      env:
        DISPLAY: :99
        CHROME_BIN: /usr/bin/chromium-browser
        CHROMEDRIVER_PATH: /usr/bin/chromedriver
      run: |
        # Start virtual display for browser tests
        Xvfb :99 -screen 0 1920x1080x24 &
        
        pytest tests/integration/ \
          --html=tests/reports/integration-test-report.html \
          --self-contained-html \
          --json-report --json-report-file=tests/reports/integration-report.json \
          --timeout=600 \
          --maxfail=10 \
          -v -s
    
    - name: Upload integration test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-reports
        path: tests/reports/

  # Job 4: Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'performance')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-perf-pip-${{ hashFiles('requirements*.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Create test reports directory
      run: mkdir -p tests/reports
    
    - name: Run performance tests
      run: |
        pytest tests/enhanced/test_performance_benchmarks.py \
          --benchmark-only \
          --benchmark-json=tests/reports/performance-benchmarks.json \
          --benchmark-histogram=tests/reports/performance-histogram.svg \
          --html=tests/reports/performance-test-report.html \
          --self-contained-html \
          --timeout=1200 \
          -v
    
    - name: Performance regression check
      run: |
        # Compare with baseline performance metrics (if available)
        python scripts/performance_comparison.py \
          --current=tests/reports/performance-benchmarks.json \
          --baseline=.github/performance-baseline.json \
          --threshold=0.15 || true
    
    - name: Upload performance reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-reports
        path: tests/reports/

  # Job 5: Security Tests
  security-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-security-pip-${{ hashFiles('requirements*.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Create test reports directory
      run: mkdir -p tests/reports
    
    - name: Run security validation tests
      run: |
        pytest tests/enhanced/test_security_validation.py \
          --html=tests/reports/security-test-report.html \
          --self-contained-html \
          --json-report --json-report-file=tests/reports/security-report.json \
          --timeout=300 \
          -v -m security
    
    - name: Upload security test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-reports
        path: tests/reports/

  # Job 6: E2E Smoke Tests
  smoke-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          chromium-browser \
          chromium-chromedriver \
          xvfb
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-smoke-pip-${{ hashFiles('requirements*.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Create test reports directory
      run: mkdir -p tests/reports
    
    - name: Run smoke tests
      env:
        DISPLAY: :99
        CHROME_BIN: /usr/bin/chromium-browser
        CHROMEDRIVER_PATH: /usr/bin/chromedriver
        ENABLE_REAL_REQUESTS: "false"  # Keep smoke tests fast
      run: |
        # Start virtual display
        Xvfb :99 -screen 0 1920x1080x24 &
        
        # Run quick smoke test
        python scripts/run_smoke_scrape.py
        
        # Run smoke test markers
        pytest -m smoke \
          --html=tests/reports/smoke-test-report.html \
          --self-contained-html \
          --timeout=180 \
          --maxfail=3 \
          -v
    
    - name: Upload smoke test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: smoke-test-reports
        path: tests/reports/

  # Job 7: Test Results Summary
  test-summary:
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, performance-tests, security-tests, smoke-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all test reports
      uses: actions/download-artifact@v3
      with:
        path: collected-reports
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install report processing dependencies
      run: |
        pip install jinja2 matplotlib seaborn pandas
    
    - name: Generate comprehensive test report
      run: |
        python scripts/generate_test_summary.py \
          --reports-dir collected-reports \
          --output-dir final-reports \
          --format html,json
    
    - name: Upload final test summary
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-summary
        path: final-reports/
    
    - name: Post test summary comment (on PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'final-reports/summary.json';
          
          if (fs.existsSync(path)) {
            const summary = JSON.parse(fs.readFileSync(path, 'utf8'));
            
            const comment = `## üß™ Test Results Summary
            
            | Test Suite | Status | Coverage | Duration |
            |-----------|--------|----------|----------|
            | Unit Tests | ${summary.unit_tests?.status || 'N/A'} | ${summary.unit_tests?.coverage || 'N/A'}% | ${summary.unit_tests?.duration || 'N/A'} |
            | Integration | ${summary.integration?.status || 'N/A'} | N/A | ${summary.integration?.duration || 'N/A'} |
            | Performance | ${summary.performance?.status || 'N/A'} | N/A | ${summary.performance?.duration || 'N/A'} |
            | Security | ${summary.security?.status || 'N/A'} | N/A | ${summary.security?.duration || 'N/A'} |
            | Smoke Tests | ${summary.smoke?.status || 'N/A'} | N/A | ${summary.smoke?.duration || 'N/A'} |
            
            **Overall Status**: ${summary.overall_status || 'Unknown'}
            **Total Duration**: ${summary.total_duration || 'N/A'}
            
            ${summary.recommendations ? '### üìã Recommendations\n' + summary.recommendations.map(r => `- ${r}`).join('\n') : ''}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  # Job 8: Quality Gates
  quality-gates:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download test reports
      uses: actions/download-artifact@v3
      with:
        path: reports
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Check quality gates
      run: |
        python scripts/quality_gates.py \
          --coverage-threshold 80 \
          --test-pass-rate 95 \
          --security-issues-max 0 \
          --reports-dir reports
    
    - name: Fail if quality gates not met
      if: failure()
      run: |
        echo "‚ùå Quality gates not met. Check the quality gates report."
        exit 1

# Workflow dispatch handling for different test types
  selective-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.test_type != 'all'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    
    - name: Run selective tests
      run: |
        case "${{ github.event.inputs.test_type }}" in
          unit)
            pytest tests/unit/ -v
            ;;
          integration)
            pytest tests/integration/ -v
            ;;
          performance)
            pytest tests/enhanced/test_performance_benchmarks.py -v
            ;;
          security)
            pytest tests/enhanced/test_security_validation.py -v
            ;;
          smoke)
            pytest -m smoke -v
            ;;
          *)
            echo "Unknown test type: ${{ github.event.inputs.test_type }}"
            exit 1
            ;;
        esac